import streamlit as st
from PyPDF2 import PdfReader
from difflib import SequenceMatcher
import base64
import re
import requests
import jieba
import hashlib
import time
from functools import lru_cache
from collections import defaultdict

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡
st.set_page_config(
    page_title="Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·",
    page_icon="ğŸ“„",
    layout="wide"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .stApp { max-width: 1200px; margin: 0 auto; }
    .stFileUploader { width: 100%; }
    .highlight-conflict { background-color: #ffeeba; padding: 2px 4px; border-radius: 3px; }
    .clause-box { border-left: 4px solid #007bff; padding: 10px; margin: 10px 0; background-color: #f8f9fa; }
    .compliance-ok { border-left: 4px solid #28a745; }
    .compliance-warning { border-left: 4px solid #ffc107; }
    .compliance-conflict { border-left: 4px solid #dc3545; }
    .model-response { background-color: #f0f2f6; padding: 15px; border-radius: 5px; margin: 10px 0; }
    .processing-bar { background-color: #e9ecef; border-radius: 5px; padding: 3px; margin: 10px 0; }
    .processing-progress { background-color: #007bff; height: 10px; border-radius: 3px; }
    .section-header { background-color: #f8f9fa; padding: 10px; border-radius: 5px; margin: 15px 0; }
</style>
""", unsafe_allow_html=True)

# é…ç½®Qwen APIå‚æ•°
QWEN_API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"

# ç¼“å­˜ç®¡ç†
cache = defaultdict(dict)

def get_cache_key(*args):
    """ç”Ÿæˆç¼“å­˜é”®"""
    return hashlib.md5(str(args).encode()).hexdigest()

def cached_func(func):
    """å‡½æ•°ç¼“å­˜è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        key = get_cache_key(args, kwargs)
        if key in cache[func.__name__]:
            return cache[func.__name__][key]
        result = func(*args, **kwargs)
        cache[func.__name__][key] = result
        return result
    return wrapper

@cached_func
def call_qwen_api(prompt, api_key, retry=3):
    """è°ƒç”¨Qwenå¤§æ¨¡å‹APIï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    if not api_key:
        st.error("Qwen APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·åœ¨å·¦ä¾§æ è¾“å…¥å¯†é’¥")
        return None
        
    try:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        data = {
            "model": "qwen-plus",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 4000
        }
        
        # å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨
        for attempt in range(retry):
            try:
                response = requests.post(
                    QWEN_API_URL,
                    headers=headers,
                    json=data,
                    timeout=60
                )
                
                if response.status_code == 200:
                    response_json = response.json()
                    if "choices" in response_json and len(response_json["choices"]) > 0:
                        return response_json["choices"][0]["message"]["content"]
                    else:
                        st.warning(f"APIè¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ (å°è¯• {attempt+1}/{retry})")
                else:
                    st.warning(f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code} (å°è¯• {attempt+1}/{retry})")
                
                time.sleep(2 **attempt)  # æŒ‡æ•°é€€é¿
                
            except requests.exceptions.Timeout:
                st.warning(f"APIè¯·æ±‚è¶…æ—¶ (å°è¯• {attempt+1}/{retry})")
                time.sleep(2** attempt)
            except Exception as e:
                st.warning(f"APIè°ƒç”¨å¼‚å¸¸: {str(e)} (å°è¯• {attempt+1}/{retry})")
                time.sleep(2 **attempt)
                
        st.error("APIè°ƒç”¨å¤šæ¬¡å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")
        return None
        
    except Exception as e:
        st.error(f"è°ƒç”¨Qwen APIå¤±è´¥: {str(e)}")
        return None

def extract_text_from_pdf(file, progress_bar=None):
    """ä»PDFæå–æ–‡æœ¬ï¼Œæ”¯æŒå¤§æ–‡ä»¶å¤„ç†å’Œè¿›åº¦æ˜¾ç¤º"""
    try:
        pdf_reader = PdfReader(file)
        text = ""
        total_pages = len(pdf_reader.pages)
        
        for i, page in enumerate(pdf_reader.pages):
            page_text = page.extract_text() or ""
            # å¤„ç†ä¸­æ–‡ç©ºæ ¼å’Œæ¢è¡Œé—®é¢˜
            page_text = page_text.replace("  ", "").replace("\n", "").replace("\r", "")
            text += page_text
            
            # æ›´æ–°è¿›åº¦æ¡
            if progress_bar is not None:
                progress = (i + 1) / total_pages
                progress_bar.progress(progress)
                progress_bar.text(f"æå–æ–‡æœ¬: ç¬¬ {i+1}/{total_pages} é¡µ")
        
        return text
    except Exception as e:
        st.error(f"æå–æ–‡æœ¬å¤±è´¥: {str(e)}")
        return ""

def split_into_clauses(text, doc_name="æ–‡æ¡£"):
    """å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ¡æ¬¾ï¼Œå¢å¼ºä¸­æ–‡æ¡æ¬¾è¯†åˆ«å’Œå¤§æ–‡æ¡£å¤„ç†"""
    # å¢å¼ºä¸­æ–‡æ¡æ¬¾æ¨¡å¼è¯†åˆ«ï¼Œæ›´å…¨é¢çš„æ¨¡å¼åº“
    patterns = [
        # ä¸»è¦æ¡æ¬¾æ¨¡å¼
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡(?:ä¹‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)?\s*[:ï¼š]?\s*.*?)(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡(?:ä¹‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)?\s*[:ï¼š]?\s*|$)',
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ã€\s*.*?)(?=[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ã€\s*|$)',
        r'(\d+\.\s*.*?)(?=\d+\.\s*|$)',
        r'(\(\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*\)\s*.*?)(?=\(\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*\)\s*|$)',
        r'(\(\s*[1-9]+\d*\s*\)\s*.*?)(?=\(\s*[1-9]+\d*\s*\)\s*|$)',
        r'([ï¼¡-ï¼ºï½-ï½š]\.\s*.*?)(?=[ï¼¡-ï¼ºï½-ï½š]\.\s*|$)',
        r'(ã€[^ã€‘]+ã€‘\s*.*?)(?=ã€[^ã€‘]+ã€‘\s*|$)',
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¬¾\s*.*?)(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¬¾\s*|$)',
    ]
    
    # å°è¯•å„ç§æ¨¡å¼ï¼Œæ‰¾åˆ°æœ€ä½³åˆ†å‰²
    best_clauses = []
    for pattern in patterns:
        clauses = re.findall(pattern, text, re.DOTALL)
        # è¿‡æ»¤è¿‡çŸ­æ¡æ¬¾
        clauses = [clause.strip() for clause in clauses if clause.strip() and len(clause.strip()) > 10]
        if len(clauses) > len(best_clauses) and len(clauses) > 2:
            best_clauses = clauses
    
    # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿçš„æ¡æ¬¾ï¼Œè¿”å›ç»“æœ
    if best_clauses:
        return best_clauses
    
    # å°è¯•æ®µè½åˆ†å‰²ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
    paragraphs = re.split(r'[ã€‚ï¼›ï¼ï¼Ÿ]\s*', text)
    paragraphs = [p.strip() for p in paragraphs if p.strip() and len(p) > 10]
    
    # å¦‚æœæ®µè½æ•°é‡ä»ç„¶å¾ˆå°‘ï¼Œå°è¯•æŒ‰å›ºå®šé•¿åº¦åˆ†å—ï¼ˆå¤„ç†éå¸¸å¤§çš„æ–‡æ¡£ï¼‰
    if len(paragraphs) < 3 and len(text) > 5000:
        chunk_size = 1000  # æ¯ä¸ªå—å¤§çº¦1000å­—ç¬¦
        paragraphs = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
        st.warning(f"{doc_name} æ¡æ¬¾ç»“æ„ä¸æ˜æ˜¾ï¼Œå·²æŒ‰ {chunk_size} å­—ç¬¦é•¿åº¦åˆ†å—å¤„ç†")
    
    return paragraphs

@lru_cache(maxsize=1000)
def chinese_text_similarity(text1, text2):
    """è®¡ç®—ä¸­æ–‡æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œä½¿ç”¨åˆ†è¯ååŒ¹é…ï¼Œç»“æœç¼“å­˜"""
    # è¿‡æ»¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
    text1_clean = re.sub(r'[^\w\s]', '', text1)
    text2_clean = re.sub(r'[^\w\s]', '', text2)
    
    # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
    words1 = list(jieba.cut(text1_clean))
    words2 = list(jieba.cut(text2_clean))
    
    # è®¡ç®—åˆ†è¯åçš„ç›¸ä¼¼åº¦
    return SequenceMatcher(None, words1, words2).ratio()

def extract_key_terms(text):
    """æå–æ–‡æœ¬ä¸­çš„å…³é”®æœ¯è¯­ï¼Œç”¨äºå¢å¼ºåŒ¹é…"""
    terms = set()
    
    # æå–æ¡æ¬¾å·
    clause_numbers = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡', text)
    terms.update(clause_numbers)
    
    # æå–å¯èƒ½çš„å…³é”®åè¯
    nouns = re.findall(r'ã€[^ã€‘]+ã€‘', text)
    terms.update(nouns)
    
    return terms

def match_clauses(clauses1, clauses2, progress_container=None):
    """åŒ¹é…ä¸¤ä¸ªæ–‡æ¡£ä¸­çš„ç›¸ä¼¼æ¡æ¬¾ï¼Œä¼˜åŒ–ä¸­æ–‡åŒ¹é…å’Œå¤§æ–‡æ¡£å¤„ç†"""
    # é¢„å…ˆè®¡ç®—æ‰€æœ‰æ¡æ¬¾çš„å…³é”®æœ¯è¯­
    terms1 = [extract_key_terms(clause) for clause in clauses1]
    terms2 = [extract_key_terms(clause) for clause in clauses2]
    
    # å…ˆåŸºäºå…³é”®æœ¯è¯­è¿›è¡Œåˆæ­¥åŒ¹é…
    term_matches = defaultdict(list)
    for i, terms in enumerate(terms1):
        if terms:
            for j, other_terms in enumerate(terms2):
                overlap = len(terms & other_terms)
                if overlap > 0:
                    term_matches[i].append((j, overlap))
    
    matched_pairs = []
    used_indices = set()
    total = len(clauses1)
    
    for i, clause1 in enumerate(clauses1):
        # æ›´æ–°è¿›åº¦
        if progress_container is not None:
            progress = (i + 1) / total
            with progress_container:
                st.progress(progress)
                st.text(f"åŒ¹é…æ¡æ¬¾: {i+1}/{total}")
        
        best_match = None
        best_ratio = 0.25  # åŸºç¡€é˜ˆå€¼
        best_j = -1
        
        # ä¼˜å…ˆè€ƒè™‘æœ‰å…³é”®æœ¯è¯­åŒ¹é…çš„æ¡æ¬¾
        candidates = []
        if i in term_matches:
            # æŒ‰æœ¯è¯­é‡å åº¦æ’åº
            for j, _ in sorted(term_matches[i], key=lambda x: x[1], reverse=True):
                if j not in used_indices:
                    candidates.append(j)
        
        # å¦‚æœæ²¡æœ‰æœ¯è¯­åŒ¹é…ï¼Œè€ƒè™‘æ‰€æœ‰æœªåŒ¹é…çš„æ¡æ¬¾
        if not candidates:
            candidates = [j for j in range(len(clauses2)) if j not in used_indices]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        for j in candidates:
            ratio = chinese_text_similarity(clause1, clauses2[j])
            
            # å¦‚æœæœ‰å…³é”®æœ¯è¯­åŒ¹é…ï¼Œé€‚å½“æé«˜ç›¸ä¼¼åº¦åˆ†æ•°
            if i in term_matches and any(j == k for k, _ in term_matches[i]):
                ratio = min(1.0, ratio * 1.1)  # æé«˜10%
                
            if ratio > best_ratio:
                best_ratio = ratio
                best_match = clauses2[j]
                best_j = j
        
        if best_match:
            matched_pairs.append({
                "base_clause": clause1,
                "compare_clause": best_match,
                "similarity": best_ratio,
                "base_index": i,
                "compare_index": best_j
            })
            used_indices.add(best_j)
    
    # å¤„ç†æœªåŒ¹é…çš„æ¡æ¬¾
    for j in range(len(clauses2)):
        if j not in used_indices:
            matched_pairs.append({
                "base_clause": None,
                "compare_clause": clauses2[j],
                "similarity": 0,
                "base_index": -1,
                "compare_index": j
            })
    
    return matched_pairs

def analyze_compliance(base_clause, compare_clause, api_key):
    """åˆ†ææ¡æ¬¾åˆè§„æ€§"""
    if not base_clause:
        return "æ— å¯¹åº”åŸºå‡†æ¡æ¬¾å¯æ¯”å¯¹", "warning"
    
    prompt = f"""
    ä½œä¸ºæ³•å¾‹åˆè§„æ€§åˆ†æä¸“å®¶ï¼Œè¯·å¯¹æ¯”ä»¥ä¸‹ä¸¤ä¸ªæ¡æ¬¾çš„åˆè§„æ€§ï¼š
    
    åŸºå‡†æ¡æ¬¾ï¼š
    {base_clause}
    
    å¾…åˆ†ææ¡æ¬¾ï¼š
    {compare_clause}
    
    è¯·åˆ†æå¾…åˆ†ææ¡æ¬¾æ˜¯å¦ç¬¦åˆåŸºå‡†æ¡æ¬¾çš„è¦æ±‚ï¼ŒæŒ‡å‡ºä¸¤è€…çš„ä¸»è¦å·®å¼‚å’Œæ½œåœ¨å†²çªã€‚
    åˆ†æåº”åŒ…æ‹¬ï¼š
    1. æ¡æ¬¾æ ¸å¿ƒå†…å®¹å¯¹æ¯”
    2. ä¸»è¦å·®å¼‚ç‚¹
    3. åˆè§„æ€§åˆ¤æ–­ï¼ˆç¬¦åˆ/éƒ¨åˆ†ç¬¦åˆ/ä¸ç¬¦åˆï¼‰
    4. é£é™©æç¤ºï¼ˆå¦‚é€‚ç”¨ï¼‰
    
    è¯·ç”¨ä¸­æ–‡ç®€æ´æ˜äº†åœ°å›ç­”ï¼Œä¸è¦è¶…è¿‡300å­—ã€‚
    """
    
    response = call_qwen_api(prompt, api_key)
    
    if not response:
        return "åˆè§„æ€§åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥æˆ–ç¨åé‡è¯•", "error"
    
    # ç®€å•åˆ¤æ–­åˆè§„æ€§ç­‰çº§
    if "ä¸ç¬¦åˆ" in response:
        return response, "conflict"
    elif "éƒ¨åˆ†ç¬¦åˆ" in response:
        return response, "warning"
    else:
        return response, "ok"

def analyze_single_comparison(base_clauses, compare_clauses, base_name, compare_name, api_key, file_index):
    """åˆ†æå•ä¸ªæ–‡ä»¶å¯¹æ¯”"""
    st.subheader(f"ğŸ“Š {base_name} ä¸ {compare_name} æ¡æ¬¾å¯¹æ¯”åˆ†æ")
    
    # åˆ›å»ºè¿›åº¦å®¹å™¨
    progress_col1, progress_col2 = st.columns(2)
    
    with progress_col1:
        match_progress = st.empty()
    
    # åŒ¹é…æ¡æ¬¾
    matched_pairs = match_clauses(base_clauses, compare_clauses, match_progress)
    
    # æ¸…é™¤è¿›åº¦æ˜¾ç¤º
    match_progress.empty()
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆé«˜åˆ°ä½ï¼‰
    matched_pairs.sort(key=lambda x: x["similarity"], reverse=True)
    
    # æ˜¾ç¤ºåŒ¹é…ç»“æœ
    for i, pair in enumerate(matched_pairs):
        # ç”Ÿæˆå”¯ä¸€çš„expander keyï¼Œç¡®ä¿åœ¨æ•´ä¸ªåº”ç”¨ä¸­å”¯ä¸€
        expander_key = f"qwen_analysis_{file_index}_{i}_{hashlib.md5(str(pair).encode()).hexdigest()[:8]}"
        
        base_clause = pair["base_clause"]
        compare_clause = pair["compare_clause"]
        similarity = pair["similarity"]
        
        # æ˜¾ç¤ºæ¡æ¬¾å¯¹æ¯”
        if base_clause:
            with st.expander(f"æ¡æ¬¾å¯¹æ¯” #{i+1} (ç›¸ä¼¼åº¦: {similarity:.2f})", expanded=False):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown(f"**{base_name} æ¡æ¬¾**")
                    st.markdown(f'<div class="clause-box">{" ".join(base_clause[:500])}{"..." if len(base_clause) > 500 else ""}</div>', unsafe_allow_html=True)
                
                with col2:
                    st.markdown(f"**{compare_name} æ¡æ¬¾**")
                    st.markdown(f'<div class="clause-box">{" ".join(compare_clause[:500])}{"..." if len(compare_clause) > 500 else ""}</div>', unsafe_allow_html=True)
                
                # åˆè§„æ€§åˆ†æ
                analysis, status = analyze_compliance(base_clause, compare_clause, api_key)
                
                # ä½¿ç”¨å”¯ä¸€keyåˆ›å»ºåˆ†æexpander
                with st.expander("æŸ¥çœ‹Qwenå¤§æ¨¡å‹åˆè§„æ€§åˆ†æ", expanded=False, key=expander_key):
                    status_class = f"compliance-{status}"
                    st.markdown(f'<div class="model-response {status_class}">{analysis}</div>', unsafe_allow_html=True)
        else:
            with st.expander(f"ä»…åœ¨ {compare_name} ä¸­å­˜åœ¨çš„æ¡æ¬¾ #{i+1}", expanded=False):
                st.markdown(f"**{compare_name} æ¡æ¬¾**")
                st.markdown(f'<div class="clause-box compliance-warning">{" ".join(compare_clause[:500])}{"..." if len(compare_clause) > 500 else ""}</div>', unsafe_allow_html=True)

def main():
    """ä¸»å‡½æ•°"""
    st.title("ğŸ“„ Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·")
    st.write("ä¸Šä¼ åŸºå‡†PDFæ–‡æ¡£å’Œå¾…æ¯”è¾ƒPDFæ–‡æ¡£ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨åˆ†ææ¡æ¬¾åˆè§„æ€§å¹¶ç”ŸæˆæŠ¥å‘Š")
    
    # ä¾§è¾¹æ è®¾ç½®
    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")
        qwen_api_key = st.text_input("Qwen API å¯†é’¥", type="password", help="è¯·è¾“å…¥é˜¿é‡Œäº‘Qwen APIå¯†é’¥")
        st.markdown("---")
        st.header("ğŸ“ ä¸Šä¼ æ–‡æ¡£")
        base_file = st.file_uploader("ä¸Šä¼ åŸºå‡†PDFæ–‡æ¡£", type="pdf", key="base_file")
        compare_files = st.file_uploader("ä¸Šä¼ å¾…æ¯”è¾ƒPDFæ–‡æ¡£ï¼ˆå¯å¤šä¸ªï¼‰", type="pdf", accept_multiple_files=True, key="compare_files")
        st.markdown("---")
        st.info("å·¥å…·è¯´æ˜ï¼š\n1. ä¸Šä¼ åŸºå‡†æ–‡æ¡£å’Œå¾…æ¯”è¾ƒæ–‡æ¡£\n2. ç³»ç»Ÿä¼šè‡ªåŠ¨æå–æ–‡æœ¬å¹¶åˆ†å‰²æ¡æ¬¾\n3. å¯¹æ¯”åˆ†ææ¡æ¬¾ç›¸ä¼¼åº¦å’Œåˆè§„æ€§\n4. å±•ç¤ºAIåˆ†æç»“æœ")
    
    # ä¸»é€»è¾‘
    if base_file and compare_files:
        # æå–åŸºå‡†æ–‡æ¡£æ–‡æœ¬
        with st.spinner("æ­£åœ¨æå–åŸºå‡†æ–‡æ¡£æ–‡æœ¬..."):
            base_progress = st.empty()
            base_text = extract_text_from_pdf(base_file, base_progress)
            base_progress.empty()
            
            if not base_text:
                st.error("æ— æ³•ä»åŸºå‡†æ–‡æ¡£ä¸­æå–æ–‡æœ¬ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ")
                return
            
            base_name = base_file.name.split(".")[0]
            base_clauses = split_into_clauses(base_text, base_name)
            st.success(f"åŸºå‡†æ–‡æ¡£ '{base_name}' å¤„ç†å®Œæˆï¼Œæå–åˆ° {len(base_clauses)} ä¸ªæ¡æ¬¾")
        
        # å¤„ç†æ¯ä¸ªå¾…æ¯”è¾ƒæ–‡æ¡£
        for i, compare_file in enumerate(compare_files):
            with st.spinner(f"æ­£åœ¨å¤„ç†å¾…æ¯”è¾ƒæ–‡æ¡£: {compare_file.name}..."):
                compare_progress = st.empty()
                compare_text = extract_text_from_pdf(compare_file, compare_progress)
                compare_progress.empty()
                
                if not compare_text:
                    st.error(f"æ— æ³•ä»æ–‡æ¡£ '{compare_file.name}' ä¸­æå–æ–‡æœ¬ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ")
                    continue
                
                compare_name = compare_file.name.split(".")[0]
                compare_clauses = split_into_clauses(compare_text, compare_name)
                st.success(f"æ–‡æ¡£ '{compare_name}' å¤„ç†å®Œæˆï¼Œæå–åˆ° {len(compare_clauses)} ä¸ªæ¡æ¬¾")
                
                # åˆ†æå¯¹æ¯”
                analyze_single_comparison(
                    base_clauses,
                    compare_clauses,
                    base_name,
                    compare_name,
                    qwen_api_key,
                    file_index=i  # ä¼ å…¥æ–‡ä»¶ç´¢å¼•ä½œä¸ºå”¯ä¸€æ ‡è¯†
                )
                st.markdown("---")
    
    elif base_file is None and compare_files:
        st.warning("è¯·å…ˆä¸Šä¼ åŸºå‡†PDFæ–‡æ¡£")
    elif base_file and not compare_files:
        st.warning("è¯·ä¸Šä¼ è‡³å°‘ä¸€ä¸ªå¾…æ¯”è¾ƒPDFæ–‡æ¡£")

if __name__ == "__main__":
    main()
