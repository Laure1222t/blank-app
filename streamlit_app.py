import streamlit as st
from PyPDF2 import PdfReader
from difflib import SequenceMatcher
import base64
import re
import requests
import jieba  # ç”¨äºä¸­æ–‡åˆ†è¯ï¼Œæé«˜åŒ¹é…ç²¾åº¦

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡
st.set_page_config(
    page_title="Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·",
    page_icon="ğŸ“„",
    layout="wide"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .stApp { max-width: 1200px; margin: 0 auto; }
    .stFileUploader { width: 100%; }
    .highlight-conflict { background-color: #ffeeba; padding: 2px 4px; border-radius: 3px; }
    .clause-box { border-left: 4px solid #007bff; padding: 10px; margin: 10px 0; background-color: #f8f9fa; }
    .compliance-ok { border-left: 4px solid #28a745; }
    .compliance-warning { border-left: 4px solid #ffc107; }
    .compliance-conflict { border-left: 4px solid #dc3545; }
    .model-response { background-color: #f0f2f6; padding: 15px; border-radius: 5px; margin: 10px 0; }
</style>
""", unsafe_allow_html=True)

# é…ç½®Qwen APIå‚æ•° - ä½¿ç”¨æŒ‡å®šçš„APIé“¾æ¥
QWEN_API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"

def call_qwen_api(prompt, api_key):
    """è°ƒç”¨Qwenå¤§æ¨¡å‹APIï¼Œä½¿ç”¨æŒ‡å®šçš„APIé“¾æ¥"""
    if not api_key:
        st.error("Qwen APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·åœ¨å·¦ä¾§æ è¾“å…¥å¯†é’¥")
        return None
        
    try:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        # æ„å»ºç¬¦åˆAPIè¦æ±‚çš„è¯·æ±‚æ•°æ®
        data = {
            "model": "qwen-plus",  # å¯æ ¹æ®éœ€è¦æ›´æ¢ä¸ºå…¶ä»–Qwenæ¨¡å‹å¦‚qwen-max
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 5000
        }
        
        # ä½¿ç”¨æŒ‡å®šçš„APIé“¾æ¥å‘é€POSTè¯·æ±‚
        response = requests.post(
            QWEN_API_URL,
            headers=headers,
            json=data,
            timeout=60
        )
        
        # æ£€æŸ¥HTTPå“åº”çŠ¶æ€
        if response.status_code != 200:
            st.error(f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}ï¼Œå“åº”: {response.text}")
            return None
            
        # è§£æJSONå“åº”
        response_json = response.json()
        
        # æ£€æŸ¥å“åº”ç»“æ„
        if "choices" not in response_json or len(response_json["choices"]) == 0:
            st.error("APIè¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")
            return None
            
        return response_json["choices"][0]["message"]["content"]
        
    except requests.exceptions.Timeout:
        st.error("APIè¯·æ±‚è¶…æ—¶ï¼Œè¯·é‡è¯•")
        return None
    except Exception as e:
        st.error(f"è°ƒç”¨Qwen APIå¤±è´¥: {str(e)}")
        return None

def extract_text_from_pdf(file):
    """ä»PDFæå–æ–‡æœ¬ï¼Œä¼˜åŒ–ä¸­æ–‡å¤„ç†"""
    try:
        pdf_reader = PdfReader(file)
        text = ""
        for page in pdf_reader.pages:
            page_text = page.extract_text() or ""
            # å¤„ç†ä¸­æ–‡ç©ºæ ¼å’Œæ¢è¡Œé—®é¢˜
            page_text = page_text.replace("  ", "").replace("\n", "").replace("\r", "")
            text += page_text
        return text
    except Exception as e:
        st.error(f"æå–æ–‡æœ¬å¤±è´¥: {str(e)}")
        return ""

def split_into_clauses(text):
    """å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ¡æ¬¾ï¼Œå¢å¼ºä¸­æ–‡æ¡æ¬¾è¯†åˆ«"""
    # å¢å¼ºä¸­æ–‡æ¡æ¬¾æ¨¡å¼è¯†åˆ«
    patterns = [
        # ä¸­æ–‡æ¡æ¬¾å¸¸è§æ ¼å¼
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡\s+.*?)(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡\s+|$)',  # ç¬¬ä¸€æ¡ã€ç¬¬äºŒæ¡æ ¼å¼
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s+.*?)(?=[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s+|$)',  # ä¸€ã€äºŒã€ä¸‰ã€æ ¼å¼
        r'(\d+\.\s+.*?)(?=\d+\.\s+|$)',  # 1. 2. 3. æ ¼å¼
        r'(\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s+.*?)(?=\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s+|$)',  # (ä¸€) (äºŒ) æ ¼å¼
        r'(\([1-9]+\)\s+.*?)(?=\([1-9]+\)\s+|$)',  # (1) (2) æ ¼å¼
        r'(ã€[^\ã€‘]+ã€‘\s+.*?)(?=ã€[^\ã€‘]+ã€‘\s+|$)'  # ã€æ ‡é¢˜ã€‘æ ¼å¼
    ]
    
    for pattern in patterns:
        clauses = re.findall(pattern, text, re.DOTALL)
        if len(clauses) > 3:  # ç¡®ä¿æ‰¾åˆ°è¶³å¤Ÿå¤šçš„æ¡æ¬¾
            return [clause.strip() for clause in clauses if clause.strip()]
    
    # æŒ‰ä¸­æ–‡æ ‡ç‚¹åˆ†å‰²æ®µè½
    paragraphs = re.split(r'[ã€‚ï¼›ï¼ï¼Ÿ]\s*', text)
    paragraphs = [p.strip() for p in paragraphs if p.strip() and len(p) > 10]  # è¿‡æ»¤è¿‡çŸ­å†…å®¹
    return paragraphs

def chinese_text_similarity(text1, text2):
    """è®¡ç®—ä¸­æ–‡æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œä½¿ç”¨åˆ†è¯ååŒ¹é…"""
    # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
    words1 = list(jieba.cut(text1))
    words2 = list(jieba.cut(text2))
    
    # è®¡ç®—åˆ†è¯åçš„ç›¸ä¼¼åº¦
    return SequenceMatcher(None, words1, words2).ratio()

def match_clauses_with_multiple(reference_clauses, other_clauses_list, other_filenames):
    """å°†å‚è€ƒæ–‡æ¡£æ¡æ¬¾ä¸å¤šä¸ªå…¶ä»–æ–‡æ¡£æ¡æ¬¾è¿›è¡ŒåŒ¹é…"""
    all_matched_pairs = []  # å­˜å‚¨æ ¼å¼: (å‚è€ƒæ¡æ¬¾, å…¶ä»–æ–‡æ¡£æ¡æ¬¾, ç›¸ä¼¼åº¦, å…¶ä»–æ–‡æ¡£åç§°)
    all_used_indices = [set() for _ in other_clauses_list]  # æ¯ä¸ªæ–‡æ¡£ç»´æŠ¤ä¸€ä¸ªå·²ä½¿ç”¨æ¡æ¬¾ç´¢å¼•é›†åˆ
    
    for ref_clause in reference_clauses:
        best_matches = []  # å­˜å‚¨æ¯ä¸ªæ–‡æ¡£çš„æœ€ä½³åŒ¹é…
        
        # ä¸ºæ¯ä¸ªæ¯”è¾ƒæ–‡æ¡£æ‰¾åˆ°æœ€ä½³åŒ¹é…
        for doc_idx, (other_clauses, used_indices) in enumerate(zip(other_clauses_list, all_used_indices)):
            best_match = None
            best_ratio = 0.25  # ä¸­æ–‡åŒ¹é…é˜ˆå€¼
            best_j = -1
            
            for j, other_clause in enumerate(other_clauses):
                if j not in used_indices:
                    ratio = chinese_text_similarity(ref_clause, other_clause)
                    if ratio > best_ratio:
                        best_ratio = ratio
                        best_match = other_clause
                        best_j = j
            
            if best_match:
                best_matches.append({
                    "clause": best_match,
                    "ratio": best_ratio,
                    "index": best_j,
                    "doc_name": other_filenames[doc_idx]
                })
                all_used_indices[doc_idx].add(best_j)
        
        if best_matches:
            all_matched_pairs.append({
                "reference_clause": ref_clause,
                "matches": best_matches
            })
    
    # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„æœªåŒ¹é…æ¡æ¬¾
    unmatched = []
    for doc_idx, (other_clauses, used_indices) in enumerate(zip(other_clauses_list, all_used_indices)):
        unmatched_clauses = [
            clause for j, clause in enumerate(other_clauses) 
            if j not in used_indices
        ]
        unmatched.append({
            "doc_name": other_filenames[doc_idx],
            "clauses": unmatched_clauses
        })
    
    # è®¡ç®—å‚è€ƒæ–‡æ¡£ä¸­æœªåŒ¹é…çš„æ¡æ¬¾
    matched_ref_indices = set()
    for i, match_group in enumerate(all_matched_pairs):
        if match_group["matches"]:  # å¦‚æœæœ‰ä»»ä½•åŒ¹é…
            matched_ref_indices.add(i)
    
    unmatched_reference = [
        clause for i, clause in enumerate(reference_clauses)
        if i not in matched_ref_indices
    ]
    
    return all_matched_pairs, unmatched_reference, unmatched

def create_download_link(content, filename, text):
    """ç”Ÿæˆä¸‹è½½é“¾æ¥"""
    b64 = base64.b64encode(content.encode()).decode()
    return f'<a href="data:file/txt;base64,{b64}" download="{filename}">{text}</a>'

def analyze_compliance_with_qwen_multiple(reference_clause, other_clauses_info, reference_filename, api_key):
    """ä½¿ç”¨Qwenå¤§æ¨¡å‹åˆ†æå‚è€ƒæ¡æ¬¾ä¸å¤šä¸ªå…¶ä»–æ¡æ¬¾çš„åˆè§„æ€§"""
    # æ„å»ºæ¯”è¾ƒæ¡æ¬¾éƒ¨åˆ†
    other_clauses_text = ""
    for info in other_clauses_info:
        other_clauses_text += f"\n{info['doc_name']} æ¡æ¬¾: {info['clause']} (ç›¸ä¼¼åº¦: {info['ratio']:.2%})"
    
    prompt = f"""
    è¯·ä»”ç»†åˆ†æä»¥ä¸‹å‚è€ƒæ¡æ¬¾ä¸å¤šä¸ªå…¶ä»–æ–‡æ¡£æ¡æ¬¾çš„åˆè§„æ€§ï¼Œåˆ¤æ–­å®ƒä»¬ä¹‹é—´æ˜¯å¦å­˜åœ¨å†²çªï¼š
    
    å‚è€ƒæ–‡æ¡£ ({reference_filename}) æ¡æ¬¾ï¼š{reference_clause}
    
    å…¶ä»–æ–‡æ¡£æ¡æ¬¾ï¼š{other_clauses_text}
    
    è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”¨ä¸­æ–‡è¿›è¡Œè¯¦ç»†åˆ†æï¼š
    1. ç›¸ä¼¼åº¦è¯„ä¼°ï¼šåˆ†åˆ«è¯„ä¼°å‚è€ƒæ¡æ¬¾ä¸æ¯ä¸ªå…¶ä»–æ¡æ¬¾çš„ç›¸ä¼¼ç¨‹åº¦ï¼ˆé«˜/ä¸­/ä½ï¼‰
    2. å·®å¼‚ç‚¹åˆ†æï¼šç®€è¦æŒ‡å‡ºå‚è€ƒæ¡æ¬¾ä¸æ¯ä¸ªå…¶ä»–æ¡æ¬¾åœ¨è¡¨è¿°ã€èŒƒå›´ã€è¦æ±‚ç­‰æ–¹é¢çš„ä¸»è¦å·®å¼‚
    3. åˆè§„æ€§åˆ¤æ–­ï¼šåˆ¤æ–­å‚è€ƒæ¡æ¬¾ä¸æ¯ä¸ªå…¶ä»–æ¡æ¬¾æ˜¯å¦å­˜åœ¨å†²çªï¼ˆæ— å†²çª/è½»å¾®å†²çª/ä¸¥é‡å†²çªï¼‰
    4. å†²çªåŸå› ï¼šå¦‚æœå­˜åœ¨å†²çªï¼Œè¯·å…·ä½“è¯´æ˜å†²çªçš„åŸå› å’Œå¯èƒ½å¸¦æ¥çš„å½±å“
    5. å»ºè®®ï¼šé’ˆå¯¹å‘ç°çš„é—®é¢˜ï¼Œç»™å‡ºä¸“ä¸šçš„å¤„ç†å»ºè®®
    
    åˆ†ææ—¶è¯·ç‰¹åˆ«æ³¨æ„ä¸­æ–‡æ³•å¾‹/åˆåŒæ¡æ¬¾ä¸­å¸¸ç”¨è¡¨è¿°çš„ç»†å¾®å·®åˆ«ï¼Œ
    å¦‚"åº”å½“"ä¸"å¿…é¡»"ã€"ä¸å¾—"ä¸"ç¦æ­¢"ã€"å¯ä»¥"ä¸"æœ‰æƒ"ç­‰è¯è¯­çš„åŒºåˆ«ã€‚
    """
    
    return call_qwen_api(prompt, api_key)

def analyze_standalone_clause_with_qwen(clause, doc_name, api_key):
    """ä½¿ç”¨Qwenå¤§æ¨¡å‹åˆ†æç‹¬ç«‹æ¡æ¬¾ï¼ˆæœªåŒ¹é…çš„æ¡æ¬¾ï¼‰"""
    prompt = f"""
    è¯·åˆ†æä»¥ä¸‹ä¸­æ–‡æ¡æ¬¾çš„å†…å®¹ï¼š
    
    {doc_name} ä¸­çš„æ¡æ¬¾ï¼š{clause}
    
    è¯·ç”¨ä¸­æ–‡è¯„ä¼°è¯¥æ¡æ¬¾çš„ä¸»è¦å†…å®¹ã€æ ¸å¿ƒè¦æ±‚ã€æ½œåœ¨å½±å“å’Œå¯èƒ½å­˜åœ¨çš„é—®é¢˜ï¼Œ
    å¹¶ç»™å‡ºç®€è¦åˆ†æå’Œå»ºè®®ã€‚åˆ†ææ—¶è¯·æ³¨æ„ä¸­æ–‡è¡¨è¿°çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§ã€‚
    """
    
    return call_qwen_api(prompt, api_key)

def show_compliance_analysis(reference_text, reference_filename, other_texts, other_filenames, api_key):
    """æ˜¾ç¤º1å¯¹å¤šåˆè§„æ€§åˆ†æç»“æœ"""
    # åˆ†å‰²æ¡æ¬¾
    with st.spinner("æ­£åœ¨åˆ†æä¸­æ–‡æ¡æ¬¾ç»“æ„..."):
        reference_clauses = split_into_clauses(reference_text)
        other_clauses_list = [split_into_clauses(text) for text in other_texts]
        
        st.success(f"æ¡æ¬¾åˆ†æå®Œæˆ: {reference_filename} è¯†åˆ«å‡º {len(reference_clauses)} æ¡æ¡æ¬¾")
        for doc_name, clauses in zip(other_filenames, other_clauses_list):
            st.success(f"{doc_name} è¯†åˆ«å‡º {len(clauses)} æ¡æ¡æ¬¾")
    
    # åŒ¹é…æ¡æ¬¾
    with st.spinner("æ­£åœ¨åŒ¹é…ç›¸ä¼¼æ¡æ¬¾..."):
        matched_pairs, unmatched_reference, unmatched_others = match_clauses_with_multiple(
            reference_clauses, other_clauses_list, other_filenames
        )
    
    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
    st.divider()
    stats_cols = st.columns(len(other_filenames) + 1)
    stats_cols[0].metric(f"{reference_filename} æ¡æ¬¾æ•°", len(reference_clauses))
    for i, (doc_name, clauses) in enumerate(zip(other_filenames, other_clauses_list)):
        stats_cols[i+1].metric(f"{doc_name} æ¡æ¬¾æ•°", len(clauses))
    
    st.metric("åŒ¹é…æ¡æ¬¾ç»„æ•°é‡", len(matched_pairs))
    
    # æ˜¾ç¤ºæ¡æ¬¾å¯¹æ¯”å’Œåˆè§„æ€§åˆ†æ
    st.divider()
    st.subheader("ğŸ“Š æ¡æ¬¾åˆè§„æ€§è¯¦ç»†åˆ†æï¼ˆQwenå¤§æ¨¡å‹ï¼‰")
    
    # åˆ†ææ¯ä¸ªåŒ¹é…ç»„çš„åˆè§„æ€§
    for i, match_group in enumerate(matched_pairs):
        reference_clause = match_group["reference_clause"]
        matches = match_group["matches"]
        
        st.markdown(f"### åŒ¹é…ç»„ {i+1}")
        
        # æ˜¾ç¤ºå‚è€ƒæ¡æ¬¾
        st.markdown(f'<div class="clause-box"><strong>{reference_filename} å‚è€ƒæ¡æ¬¾:</strong><br>{reference_clause}</div>', unsafe_allow_html=True)
        
        # æ˜¾ç¤ºæ‰€æœ‰åŒ¹é…çš„å…¶ä»–æ¡æ¬¾
        for match in matches:
            st.markdown(f'<div class="clause-box"><strong>{match["doc_name"]} åŒ¹é…æ¡æ¬¾ (ç›¸ä¼¼åº¦: {match["ratio"]:.2%}):</strong><br>{match["clause"]}</div>', unsafe_allow_html=True)
        
        with st.spinner("æ­£åœ¨è°ƒç”¨Qwenå¤§æ¨¡å‹è¿›è¡Œä¸­æ–‡åˆè§„æ€§åˆ†æ..."):
            analysis = analyze_compliance_with_qwen_multiple(
                reference_clause, matches, reference_filename, api_key
            )
        
        if analysis:
            st.markdown('<div class="model-response"><strong>Qwenå¤§æ¨¡å‹åˆ†æç»“æœ:</strong><br>' + analysis + '</div>', unsafe_allow_html=True)
        
        st.divider()
    
    # æœªåŒ¹é…çš„æ¡æ¬¾åˆ†æ
    st.subheader("æœªåŒ¹é…æ¡æ¬¾åˆ†æ")
    
    # å‚è€ƒæ–‡æ¡£æœªåŒ¹é…æ¡æ¬¾
    st.markdown(f"#### {reference_filename} ä¸­æœªåŒ¹é…çš„æ¡æ¬¾ ({len(unmatched_reference)})")
    for i, clause in enumerate(unmatched_reference):
        st.markdown(f'<div class="clause-box"><strong>æ¡æ¬¾ {i+1}:</strong><br>{clause}</div>', unsafe_allow_html=True)
        with st.spinner(f"æ­£åœ¨åˆ†æ {reference_filename} æœªåŒ¹é…æ¡æ¬¾ {i+1}..."):
            analysis = analyze_standalone_clause_with_qwen(clause, reference_filename, api_key)
        if analysis:
            st.markdown('<div class="model-response">' + analysis + '</div>', unsafe_allow_html=True)
        st.divider()
    
    # å…¶ä»–æ–‡æ¡£æœªåŒ¹é…æ¡æ¬¾
    for doc_unmatched in unmatched_others:
        doc_name = doc_unmatched["doc_name"]
        clauses = doc_unmatched["clauses"]
        st.markdown(f"#### {doc_name} ä¸­æœªåŒ¹é…çš„æ¡æ¬¾ ({len(clauses)})")
        for i, clause in enumerate(clauses):
            st.markdown(f'<div class="clause-box"><strong>æ¡æ¬¾ {i+1}:</strong><br>{clause}</div>', unsafe_allow_html=True)
            with st.spinner(f"æ­£åœ¨åˆ†æ {doc_name} æœªåŒ¹é…æ¡æ¬¾ {i+1}..."):
                analysis = analyze_standalone_clause_with_qwen(clause, doc_name, api_key)
            if analysis:
                st.markdown('<div class="model-response">' + analysis + '</div>', unsafe_allow_html=True)
            st.divider()

# ä¸»ç¨‹åº
def main():
    st.title("ğŸ“„ Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·")
    st.write("ä¸Šä¼ ä¸€ä¸ªå‚è€ƒPDFæ–‡æ¡£å’Œå¤šä¸ªå¾…æ¯”è¾ƒçš„PDFæ–‡æ¡£ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨åˆ†ææ¡æ¬¾åˆè§„æ€§")
    
    # ä¾§è¾¹æ è®¾ç½®
    with st.sidebar:
        st.header("ğŸ”§ è®¾ç½®")
        api_key = st.text_input("è¯·è¾“å…¥Qwen APIå¯†é’¥", type="password")
        st.markdown("""
        æç¤º: åˆ†æç»“æœåŸºäºQwenå¤§æ¨¡å‹ï¼Œä»…ä¾›å‚è€ƒã€‚
        """)
    
    # æ–‡ä»¶ä¸Šä¼ 
    col1, col2 = st.columns(2)
    with col1:
        reference_file = st.file_uploader("ä¸Šä¼ å‚è€ƒPDFæ–‡æ¡£", type="pdf", key="reference")
    
    with col2:
        other_files = st.file_uploader(
            "ä¸Šä¼ å¤šä¸ªå¾…æ¯”è¾ƒçš„PDFæ–‡æ¡£", 
            type="pdf", 
            key="others",
            accept_multiple_files=True
        )
    
    # å½“æ–‡ä»¶ä¸Šä¼ åè¿›è¡Œå¤„ç†
    if reference_file and other_files:
        # æå–å‚è€ƒæ–‡æ¡£æ–‡æœ¬
        reference_text = extract_text_from_pdf(reference_file)
        reference_filename = reference_file.name
        
        # æå–å…¶ä»–æ–‡æ¡£æ–‡æœ¬
        other_texts = []
        other_filenames = []
        for file in other_files:
            other_texts.append(extract_text_from_pdf(file))
            other_filenames.append(file.name)
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        show_compliance_analysis(
            reference_text, 
            reference_filename, 
            other_texts, 
            other_filenames, 
            api_key
        )

if __name__ == "__main__":
    main()
