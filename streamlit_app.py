import streamlit as st
import PyPDF2
import os
import tempfile
import re
import string
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
import jieba  # æ–°å¢ï¼šç”¨äºä¸­æ–‡åˆ†è¯

# ç¡®ä¿ä¸‹è½½NLTKæ‰€éœ€èµ„æº
nltk.download('stopwords', quiet=True)

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
plt.rcParams["axes.unicode_minus"] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="PDFè§£æä¸å¯¹æ¯”åˆ†æå·¥å…·",
    page_icon="ğŸ“„",
    layout="wide"
)

# åŠ è½½ä¸­æ–‡åœç”¨è¯
def load_chinese_stopwords():
    """åŠ è½½ä¸­æ–‡åœç”¨è¯"""
    # åŸºç¡€ä¸­æ–‡åœç”¨è¯
    stopwords_list = [
        "çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", 
        "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™", "ä¸", "åŠ", "ç­‰",
        "å¯ä»¥", "æˆ‘ä»¬", "å¯¹äº", "è¿›è¡Œ", "å¯èƒ½", "è¡¨ç¤º", "è®¤ä¸º", "æå‡º", "é—®é¢˜", "æ–¹æ³•", "ç ”ç©¶", "é€šè¿‡"
    ]
    
    # è¡¥å……NLTKçš„è‹±æ–‡åœç”¨è¯
    stopwords_list.extend(stopwords.words('english'))
    
    return set(stopwords_list)

# åˆå§‹åŒ–åœç”¨è¯é›†åˆ
stop_words = load_chinese_stopwords()

# æ ‡é¢˜
st.title("ğŸ“„ PDFè§£æä¸å¯¹æ¯”åˆ†æå·¥å…·")

# ä¾§è¾¹æ å¯¼èˆª
st.sidebar.title("åŠŸèƒ½å¯¼èˆª")
option = st.sidebar.selectbox(
    "é€‰æ‹©åŠŸèƒ½",
    ("PDFè§£æ", "å¤šæ–‡ä»¶å¯¹æ¯”åˆ†æ")
)

# æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
def preprocess_text(text, is_chinese=True):
    """å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼šæ”¯æŒä¸­æ–‡åˆ†è¯ã€å»é™¤æ ‡ç‚¹ã€åœç”¨è¯"""
    # è½¬æ¢ä¸ºå°å†™ï¼ˆä»…å¯¹è‹±æ–‡æœ‰æ•ˆï¼‰
    text = text.lower()
    
    # å»é™¤æ ‡ç‚¹
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # å»é™¤æ•°å­—
    text = re.sub(r'\d+', '', text)
    
    # å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    
    # åˆ†è¯ï¼šä¸­æ–‡ä½¿ç”¨jiebaï¼Œè‹±æ–‡ä½¿ç”¨ç©ºæ ¼åˆ†å‰²
    if is_chinese:
        words = jieba.cut(text)
    else:
        words = text.split()
    
    # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
    filtered_words = []
    for word in words:
        # è¿‡æ»¤æ¡ä»¶ï¼šä¸åœ¨åœç”¨è¯è¡¨ä¸­ï¼Œé•¿åº¦å¤§äº1ï¼Œä¸æ˜¯çº¯ç©ºæ ¼
        if word not in stop_words and len(word.strip()) > 1:
            filtered_words.append(word.strip())
    
    return filtered_words

# æå–PDFæ–‡æœ¬
def extract_text_from_pdf(pdf_file):
    """ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹"""
    pdf_reader = PyPDF2.PdfReader(pdf_file)
    text = ""
    for page in pdf_reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text
    return text

# åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸»è¦ä¸ºä¸­æ–‡
def is_chinese_text(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸»è¦ä¸ºä¸­æ–‡"""
    chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
    return len(chinese_chars) / len(text) > 0.3 if text else True

# åˆ†ææ–‡æœ¬å‡½æ•°
def analyze_text(text):
    """åˆ†ææ–‡æœ¬å†…å®¹ï¼Œè¿”å›ç»Ÿè®¡ä¿¡æ¯"""
    # åˆ¤æ–­æ˜¯å¦ä¸ºä¸­æ–‡æ–‡æœ¬
    chinese = is_chinese_text(text)
    
    # é¢„å¤„ç†æ–‡æœ¬
    words = preprocess_text(text, chinese)
    
    # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    total_words = len(words)
    unique_words = len(set(words))
    word_freq = Counter(words)
    top_words = word_freq.most_common(10)
    
    return {
        "total_words": total_words,
        "unique_words": unique_words,
        "top_words": top_words,
        "word_freq": word_freq,
        "words": words,
        "is_chinese": chinese
    }

# ç”Ÿæˆè¯äº‘
def generate_wordcloud(word_freq):
    """æ ¹æ®è¯é¢‘ç”Ÿæˆè¯äº‘"""
    # é…ç½®ä¸­æ–‡è¯äº‘
    wordcloud = WordCloud(
        width=800, 
        height=400, 
        background_color='white',
        font_path="simhei.ttf",  # å°è¯•ä½¿ç”¨ç³»ç»Ÿä¸­çš„é»‘ä½“å­—ä½“
        font_step=1,
        max_words=100
    ).generate_from_frequencies(word_freq)
    
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    return plt

# æ¯”è¾ƒå¤šä¸ªæ–‡æ¡£
def compare_documents(doc_analyses):
    """æ¯”è¾ƒå¤šä¸ªæ–‡æ¡£çš„åˆ†æç»“æœ"""
    # æå–æ‰€æœ‰æ–‡æ¡£çš„è¯é¢‘
    all_words = set()
    for doc in doc_analyses.values():
        all_words.update(doc["word_freq"].keys())
    
    # è®¡ç®—æ¯ä¸ªè¯åœ¨å„æ–‡æ¡£ä¸­çš„é¢‘ç‡
    word_comparison = {}
    for word in all_words:
        word_comparison[word] = {name: doc["word_freq"].get(word, 0) for name, doc in doc_analyses.items()}
    
    # æ‰¾å‡ºå…±åŒå‡ºç°çš„è¯
    common_words = [word for word, counts in word_comparison.items() if all(c > 0 for c in counts.values())]
    common_words_sorted = sorted(common_words, key=lambda w: sum(word_comparison[w].values()), reverse=True)[:10]
    
    # æ‰¾å‡ºå„æ–‡æ¡£ç‰¹æœ‰çš„è¯
    unique_words = {}
    for name, doc in doc_analyses.items():
        other_docs = [d for n, d in doc_analyses.items() if n != name]
        other_words = set()
        for d in other_docs:
            other_words.update(d["word_freq"].keys())
        unique = [word for word in doc["word_freq"] if word not in other_words]
        # æŒ‰é¢‘ç‡æ’åº
        unique_sorted = sorted(unique, key=lambda w: doc["word_freq"][w], reverse=True)[:10]
        unique_words[name] = unique_sorted
    
    return {
        "common_words": common_words_sorted,
        "unique_words": unique_words,
        "word_comparison": word_comparison
    }

# PDFè§£æåŠŸèƒ½
if option == "PDFè§£æ":
    st.header("PDFè§£æ")
    st.write("ä¸Šä¼ PDFæ–‡ä»¶ï¼Œç³»ç»Ÿå°†è§£æå¹¶åˆ†æå…¶å†…å®¹ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰")
    
    pdf_file = st.file_uploader("é€‰æ‹©PDFæ–‡ä»¶", type="pdf")
    
    if pdf_file is not None:
        # æ˜¾ç¤ºæ–‡ä»¶å
        st.write(f"å·²ä¸Šä¼ æ–‡ä»¶: {pdf_file.name}")
        
        # æå–æ–‡æœ¬
        with st.spinner("æ­£åœ¨è§£æPDFæ–‡ä»¶..."):
            text = extract_text_from_pdf(pdf_file)
        
        # æ˜¾ç¤ºæå–çš„æ–‡æœ¬ï¼ˆå‰500å­—ç¬¦ï¼‰
        st.subheader("æ–‡æœ¬é¢„è§ˆ")
        if len(text) > 0:
            st.text_area("PDFå†…å®¹é¢„è§ˆ", text[:500] + ("..." if len(text) > 500 else ""), height=200)
            
            # åˆ†ææ–‡æœ¬
            with st.spinner("æ­£åœ¨åˆ†ææ–‡æœ¬å†…å®¹..."):
                analysis = analyze_text(text)
            
            # æ˜¾ç¤ºåˆ†æç»“æœ
            st.subheader("æ–‡æœ¬åˆ†æç»“æœ")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("æ€»è¯æ•°", analysis["total_words"])
                st.metric("ç‹¬ç‰¹è¯æ•°", analysis["unique_words"])
                st.metric("è¯­è¨€ç±»å‹", "ä¸­æ–‡" if analysis["is_chinese"] else "è‹±æ–‡")
            
            with col2:
                st.write("é«˜é¢‘è¯Top 10:")
                top_words_df = {
                    "è¯è¯­": [word for word, _ in analysis["top_words"]],
                    "å‡ºç°æ¬¡æ•°": [count for _, count in analysis["top_words"]]
                }
                st.dataframe(top_words_df)
            
            # ç”Ÿæˆè¯äº‘
            st.subheader("è¯äº‘")
            wordcloud_fig = generate_wordcloud(analysis["word_freq"])
            st.pyplot(wordcloud_fig)
        else:
            st.warning("æ— æ³•ä»PDFä¸­æå–æ–‡æœ¬å†…å®¹ï¼Œå¯èƒ½æ˜¯å›¾ç‰‡å‹PDFæˆ–åŠ å¯†PDFã€‚")

# å¤šæ–‡ä»¶å¯¹æ¯”åˆ†æåŠŸèƒ½
elif option == "å¤šæ–‡ä»¶å¯¹æ¯”åˆ†æ":
    st.header("å¤šæ–‡ä»¶å¯¹æ¯”åˆ†æ")
    st.write("ä¸Šä¼ å¤šä¸ªPDFæ–‡ä»¶ï¼Œç³»ç»Ÿå°†å¯¹æ¯”åˆ†æå®ƒä»¬çš„å†…å®¹å·®å¼‚ä¸å…±æ€§ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰")
    
    pdf_files = st.file_uploader("é€‰æ‹©å¤šä¸ªPDFæ–‡ä»¶", type="pdf", accept_multiple_files=True)
    
    if len(pdf_files) >= 2:
        st.write(f"å·²ä¸Šä¼  {len(pdf_files)} ä¸ªæ–‡ä»¶")
        
        # è§£ææ‰€æœ‰æ–‡ä»¶
        doc_analyses = {}
        with st.spinner("æ­£åœ¨è§£ææ‰€æœ‰PDFæ–‡ä»¶..."):
            for file in pdf_files:
                text = extract_text_from_pdf(file)
                if text:
                    analysis = analyze_text(text)
                    doc_analyses[file.name] = analysis
                else:
                    st.warning(f"æ— æ³•è§£ææ–‡ä»¶: {file.name}ï¼Œå·²è·³è¿‡è¯¥æ–‡ä»¶")
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸¤ä¸ªå¯åˆ†æçš„æ–‡ä»¶
        if len(doc_analyses) >= 2:
            # æ¯”è¾ƒæ–‡æ¡£
            with st.spinner("æ­£åœ¨å¯¹æ¯”åˆ†ææ–‡æ¡£..."):
                comparison = compare_documents(doc_analyses)
            
            # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
            st.subheader("æ–‡æ¡£æ¯”è¾ƒç»“æœ")
            
            # æ˜¾ç¤ºå…±åŒè¯
            st.write("### å…±åŒé«˜é¢‘è¯ (Top 10)")
            if comparison["common_words"]:
                common_words_data = {
                    "è¯è¯­": comparison["common_words"],
                    **{name: [doc["word_freq"][word] for word in comparison["common_words"]] 
                       for name, doc in doc_analyses.items()}
                }
                st.dataframe(common_words_data)
                
                # ç»˜åˆ¶å…±åŒè¯å¯¹æ¯”å›¾
                fig, ax = plt.subplots(figsize=(10, 6))
                x = np.arange(len(comparison["common_words"]))
                width = 0.8 / len(doc_analyses)
                
                for i, (name, doc) in enumerate(doc_analyses.items()):
                    counts = [doc["word_freq"][word] for word in comparison["common_words"]]
                    ax.bar(x + i*width - 0.4 + width/2, counts, width, label=name)
                
                ax.set_xticks(x)
                ax.set_xticklabels(comparison["common_words"], rotation=45)
                ax.legend()
                ax.set_title("å…±åŒé«˜é¢‘è¯åœ¨å„æ–‡æ¡£ä¸­çš„å‡ºç°æ¬¡æ•°")
                st.pyplot(fig)
            else:
                st.info("æœªå‘ç°æ‰€æœ‰æ–‡æ¡£å…±åŒå‡ºç°çš„è¯è¯­")
            
            # æ˜¾ç¤ºå„æ–‡æ¡£çš„ç‹¬ç‰¹è¯
            st.write("### å„æ–‡æ¡£çš„ç‹¬ç‰¹é«˜é¢‘è¯ (Top 10)")
            for name, unique_words in comparison["unique_words"].items():
                with st.expander(f"{name} çš„ç‹¬ç‰¹è¯"):
                    if unique_words:
                        unique_words_data = {
                            "è¯è¯­": unique_words,
                            "å‡ºç°æ¬¡æ•°": [doc_analyses[name]["word_freq"][word] for word in unique_words]
                        }
                        st.dataframe(unique_words_data)
                    else:
                        st.info(f"{name} æ²¡æœ‰ç‹¬ç‰¹çš„è¯è¯­")
            
            # æ˜¾ç¤ºå„æ–‡æ¡£çš„åŸºæœ¬ç»Ÿè®¡å¯¹æ¯”
            st.write("### æ–‡æ¡£åŸºæœ¬ç»Ÿè®¡å¯¹æ¯”")
            stats_data = {
                "æ–‡æ¡£åç§°": list(doc_analyses.keys()),
                "æ€»è¯æ•°": [doc["total_words"] for doc in doc_analyses.values()],
                "ç‹¬ç‰¹è¯æ•°": [doc["unique_words"] for doc in doc_analyses.values()],
                "è¯­è¨€ç±»å‹": ["ä¸­æ–‡" if doc["is_chinese"] else "è‹±æ–‡" for doc in doc_analyses.values()]
            }
            st.dataframe(stats_data)
            
            # ç»˜åˆ¶ç»Ÿè®¡å¯¹æ¯”å›¾
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
            
            # æ€»è¯æ•°å¯¹æ¯”
            ax1.bar(stats_data["æ–‡æ¡£åç§°"], stats_data["æ€»è¯æ•°"], color='skyblue')
            ax1.set_title("æ€»è¯æ•°å¯¹æ¯”")
            ax1.tick_params(axis='x', rotation=45)
            
            # ç‹¬ç‰¹è¯æ•°å¯¹æ¯”
            ax2.bar(stats_data["æ–‡æ¡£åç§°"], stats_data["ç‹¬ç‰¹è¯æ•°"], color='lightgreen')
            ax2.set_title("ç‹¬ç‰¹è¯æ•°å¯¹æ¯”")
            ax2.tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            st.pyplot(fig)
            
            # æ˜¾ç¤ºå„æ–‡æ¡£çš„è¯äº‘
            st.write("### å„æ–‡æ¡£è¯äº‘å¯¹æ¯”")
            cols = st.columns(len(doc_analyses))
            for col, (name, doc) in zip(cols, doc_analyses.items()):
                with col:
                    st.write(f"**{name}**")
                    wordcloud_fig = generate_wordcloud(doc["word_freq"])
                    st.pyplot(wordcloud_fig)
        else:
            st.error("å¯åˆ†æçš„æ–‡ä»¶ä¸è¶³2ä¸ªï¼Œè¯·ä¸Šä¼ æ›´å¤šå¯è§£æçš„PDFæ–‡ä»¶")
    elif 0 < len(pdf_files) < 2:
        st.warning("è¯·è‡³å°‘ä¸Šä¼ 2ä¸ªPDFæ–‡ä»¶è¿›è¡Œå¯¹æ¯”åˆ†æ")
