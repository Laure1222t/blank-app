import streamlit as st
from PyPDF2 import PdfReader
from difflib import SequenceMatcher
import base64
import re
import requests
import jieba
from io import StringIO
import time
from typing import List, Tuple, Optional

# é¡µé¢è®¾ç½®
st.set_page_config(
    page_title="å¤šæ–‡ä»¶åŸºå‡†åˆè§„æ€§åˆ†æå·¥å…·",
    page_icon="ğŸ“Š",
    layout="wide"
)

# è‡ªå®šä¹‰æ ·å¼
st.markdown("""
<style>
    .stApp { max-width: 1400px; margin: 0 auto; }
    .analysis-card { border: 1px solid #e0e0e0; border-radius: 8px; padding: 15px; margin: 10px 0; }
    .conflict-highlight { background-color: #fff3cd; padding: 2px 4px; border-radius: 2px; }
    .progress-container { margin: 20px 0; }
</style>
""", unsafe_allow_html=True)

# APIé…ç½®
QWEN_API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"

# ä¼šè¯çŠ¶æ€åˆå§‹åŒ–
if 'analysis_progress' not in st.session_state:
    st.session_state.analysis_progress = 0
if 'partial_reports' not in st.session_state:
    st.session_state.partial_reports = {}
if 'current_analysis' not in st.session_state:
    st.session_state.current_analysis = None

def call_qwen_api(prompt: str, api_key: str) -> Optional[str]:
    """è°ƒç”¨APIå¹¶å®ç°é‡è¯•æœºåˆ¶"""
    retries = 2
    delay = 3
    
    for attempt in range(retries):
        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }
            
            data = {
                "model": "qwen-plus",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.2,
                "max_tokens": 1500
            }
            
            response = requests.post(
                QWEN_API_URL,
                headers=headers,
                json=data,
                timeout=60
            )
            
            if response.status_code == 200:
                response_json = response.json()
                if "choices" in response_json and len(response_json["choices"]) > 0:
                    return response_json["choices"][0]["message"]["content"]
                
        except Exception as e:
            if attempt < retries - 1:
                time.sleep(delay)
                continue
                
    return None

def extract_text_from_pdf(file) -> str:
    """ä»PDFæå–æ–‡æœ¬"""
    try:
        pdf_reader = PdfReader(file)
        text = ""
        for page in pdf_reader.pages:
            page_text = page.extract_text() or ""
            page_text = page_text.replace("  ", "").replace("\n", "").replace("\r", "")
            text += page_text
        return text
    except Exception as e:
        st.error(f"æå–æ–‡æœ¬å¤±è´¥: {str(e)}")
        return ""

def split_into_clauses(text: str, max_clauses: int = 30) -> List[str]:
    """åˆ†å‰²æ–‡æœ¬ä¸ºæ¡æ¬¾"""
    patterns = [
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡\s+.*?)(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡\s+|$)',
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s+.*?)(?=[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s+|$)',
        r'(\d+\.\s+.*?)(?=\d+\.\s+|$)',
        r'(\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s+.*?)(?=\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s+|$)',
        r'(\([1-9]+\)\s+.*?)(?=\([1-9]+\)\s+|$)',
        r'(ã€[^\ã€‘]+ã€‘\s+.*?)(?=ã€[^\ã€‘]+ã€‘\s+|$)'
    ]
    
    for pattern in patterns:
        clauses = re.findall(pattern, text, re.DOTALL)
        if len(clauses) > 3:
            return [clause.strip() for clause in clauses if clause.strip()][:max_clauses]
    
    paragraphs = re.split(r'[ã€‚ï¼›ï¼ï¼Ÿ]\s*', text)
    return [p.strip() for p in paragraphs if p.strip() and len(p) > 10][:max_clauses]

def chinese_text_similarity(text1: str, text2: str) -> float:
    """è®¡ç®—ä¸­æ–‡æ–‡æœ¬ç›¸ä¼¼åº¦"""
    words1 = list(jieba.cut(text1))
    words2 = list(jieba.cut(text2))
    return SequenceMatcher(None, words1, words2).ratio()

def match_clauses_with_base(base_clauses: List[str], target_clauses: List[str]) -> List[Tuple[str, str, float]]:
    """å°†ç›®æ ‡æ–‡ä»¶æ¡æ¬¾ä¸åŸºå‡†æ–‡ä»¶æ¡æ¬¾åŒ¹é…"""
    matched_pairs = []
    used_indices = set()
    
    for base_clause in base_clauses:
        best_match = None
        best_ratio = 0.3  # åŒ¹é…é˜ˆå€¼
        best_idx = -1
        
        for idx, target_clause in enumerate(target_clauses):
            if idx not in used_indices:
                ratio = chinese_text_similarity(base_clause, target_clause)
                if ratio > best_ratio and ratio > best_ratio:
                    best_ratio = ratio
                    best_match = target_clause
                    best_idx = idx
        
        if best_match:
            matched_pairs.append((base_clause, best_match, best_ratio))
            used_indices.add(best_idx)
    
    return matched_pairs

def analyze_compliance_with_base(base_clause: str, target_clause: str, 
                               base_name: str, target_name: str, 
                               api_key: str) -> Optional[str]:
    """åˆ†æç›®æ ‡æ¡æ¬¾ä¸åŸºå‡†æ¡æ¬¾çš„åˆè§„æ€§"""
    prompt = f"""
    è¯·ä»¥{base_name}ä¸ºåŸºå‡†ï¼Œåˆ†æä»¥ä¸‹æ¡æ¬¾çš„åˆè§„æ€§ï¼š
    
    åŸºå‡†æ¡æ¬¾ï¼ˆ{base_name}ï¼‰ï¼š{base_clause}
    
    ç›®æ ‡æ¡æ¬¾ï¼ˆ{target_name}ï¼‰ï¼š{target_clause}
    
    è¯·é‡ç‚¹åˆ†æï¼š
    1. ç›®æ ‡æ¡æ¬¾æ˜¯å¦ç¬¦åˆåŸºå‡†æ¡æ¬¾çš„è¦æ±‚
    2. å­˜åœ¨å“ªäº›åç¦»æˆ–å†²çªä¹‹å¤„ï¼ˆéœ€å…·ä½“æŒ‡å‡ºï¼‰
    3. åç¦»ç¨‹åº¦è¯„ä¼°ï¼ˆå®Œå…¨ç¬¦åˆ/è½»å¾®åç¦»/ä¸¥é‡åç¦»ï¼‰
    4. å¯¼è‡´åç¦»çš„å…³é”®åŸå› 
    5. å¦‚ä½•ä¿®æ”¹ç›®æ ‡æ¡æ¬¾ä»¥ç¬¦åˆåŸºå‡†è¦æ±‚
    
    è¯·ç”¨ä¸“ä¸šã€ç®€æ´çš„ä¸­æ–‡å›ç­”ï¼Œèšç„¦åˆè§„æ€§é—®é¢˜ã€‚
    """
    
    return call_qwen_api(prompt, api_key)

def generate_target_report(matched_pairs: List[Tuple[str, str, float]],
                          base_name: str, target_name: str,
                          api_key: str, target_index: int, total_targets: int) -> str:
    """ä¸ºå•ä¸ªç›®æ ‡æ–‡ä»¶ç”Ÿæˆä¸åŸºå‡†æ–‡ä»¶çš„å¯¹æ¯”æŠ¥å‘Š"""
    report = []
    report.append("="*60)
    report.append(f"æ¡æ¬¾åˆè§„æ€§åˆ†ææŠ¥å‘Š: {target_name} ä¸ {base_name} å¯¹æ¯”")
    report.append(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("="*60 + "\n")
    
    # æ€»ä½“ç»Ÿè®¡
    report.append(f"åˆ†ææ¦‚è¦: å…±åŒ¹é… {len(matched_pairs)} æ¡æ¡æ¬¾\n")
    report.append("-"*60 + "\n")
    
    # è¿›åº¦è·Ÿè¸ª
    progress_container = st.empty()
    total_pairs = len(matched_pairs)
    
    # åˆ†ææ¯å¯¹æ¡æ¬¾
    for i, (base_clause, target_clause, ratio) in enumerate(matched_pairs):
        # æ›´æ–°å…¨å±€è¿›åº¦ (è€ƒè™‘å¤šä¸ªç›®æ ‡æ–‡ä»¶çš„æ€»è¿›åº¦)
        global_progress = (target_index * total_pairs + i) / (total_targets * total_pairs) if total_targets > 0 else 0
        st.session_state.analysis_progress = global_progress
        progress_container.progress(global_progress)
        
        report.append(f"æ¡æ¬¾å¯¹ {i+1} (ç›¸ä¼¼åº¦: {ratio:.2%})")
        report.append(f"åŸºå‡†æ¡æ¬¾: {base_clause[:200]}...")
        report.append(f"ç›®æ ‡æ¡æ¬¾: {target_clause[:200]}...\n")
        
        # åˆè§„æ€§åˆ†æ
        with st.spinner(f"æ­£åœ¨åˆ†æ {target_name} çš„æ¡æ¬¾ {i+1}/{total_pairs}..."):
            analysis = analyze_compliance_with_base(
                base_clause, target_clause, 
                base_name, target_name, 
                api_key
            )
        
        if analysis:
            report.append("åˆè§„æ€§åˆ†æç»“æœ:")
            report.append(analysis)
        else:
            report.append("åˆè§„æ€§åˆ†æç»“æœ: æ— æ³•è·å–æœ‰æ•ˆçš„åˆ†æç»“æœ")
        
        report.append("\n" + "-"*60 + "\n")
        
        # ä¿å­˜éƒ¨åˆ†ç»“æœ
        st.session_state.partial_reports[target_name] = report.copy()
        time.sleep(1)  # æ§åˆ¶APIè°ƒç”¨é¢‘ç‡
    
    # ç›®æ ‡æ–‡ä»¶æ€»ä½“è¯„ä¼°
    if matched_pairs:
        with st.spinner(f"ç”Ÿæˆ {target_name} çš„æ€»ä½“è¯„ä¼°..."):
            summary_prompt = f"""
            åŸºäºå¯¹{target_name}ä¸åŸºå‡†æ–‡ä»¶{base_name}çš„{len(matched_pairs)}å¯¹æ¡æ¬¾çš„å¯¹æ¯”åˆ†æï¼Œ
            è¯·è¯„ä¼°{target_name}æ•´ä½“ç¬¦åˆåŸºå‡†çš„ç¨‹åº¦ï¼ŒåŒ…æ‹¬ï¼š
            1. æ€»ä½“åˆè§„æ€§è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰åŠç†ç”±
            2. æœ€ä¸»è¦çš„ä¸åˆè§„ç‚¹
            3. æ•´ä½“ä¿®æ”¹å»ºè®®
            """
            summary = call_qwen_api(summary_prompt, api_key)
            
            if summary:
                report.append("="*60)
                report.append(f"{target_name} ä¸ {base_name} æ€»ä½“åˆè§„æ€§è¯„ä¼°")
                report.append("="*60)
                report.append(summary)
    
    return "\n".join(report)

def generate_combined_summary(reports: dict, base_name: str, api_key: str) -> Optional[str]:
    """ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶ä¸åŸºå‡†å¯¹æ¯”çš„ç»¼åˆæ‘˜è¦"""
    if not reports:
        return None
        
    target_names = list(reports.keys())
    summary_prompt = f"""
    ä»¥ä¸‹æ˜¯{len(target_names)}ä¸ªæ–‡ä»¶ä¸åŸºå‡†æ–‡ä»¶{base_name}çš„åˆè§„æ€§åˆ†æç»“æœæ‘˜è¦ã€‚
    è¯·ç»¼åˆè¿™äº›ç»“æœï¼Œç”Ÿæˆä¸€ä»½æ€»ä½“æ‘˜è¦æŠ¥å‘Šï¼š
    
    """
    
    # ä¸ºæ¯ä¸ªç›®æ ‡æ–‡ä»¶æ·»åŠ å…³é”®ä¿¡æ¯
    for name, report in reports.items():
        summary_prompt += f"æ–‡ä»¶ {name} çš„åˆ†æè¦ç‚¹ï¼š\n"
        summary_prompt += f"{report[:1000]}...\n\n"  # å–æŠ¥å‘Šå¼€å¤´éƒ¨åˆ†ä½œä¸ºæ‘˜è¦ä¾æ®
    
    summary_prompt += """
    è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆç»¼åˆè¯„ä¼°ï¼š
    1. æ‰€æœ‰æ–‡ä»¶çš„æ•´ä½“åˆè§„æ€§å¯¹æ¯”
    2. å„æ–‡ä»¶å…±åŒå­˜åœ¨çš„åˆè§„æ€§é—®é¢˜
    3. å„æ–‡ä»¶ç‰¹æœ‰çš„åˆè§„æ€§é—®é¢˜
    4. é’ˆå¯¹æ‰€æœ‰æ–‡ä»¶çš„ä¼˜å…ˆçº§ä¿®æ”¹å»ºè®®
    """
    
    return call_qwen_api(summary_prompt, api_key)

def get_download_link(text: str, filename: str) -> str:
    """ç”ŸæˆæŠ¥å‘Šä¸‹è½½é“¾æ¥"""
    b64 = base64.b64encode(text.encode()).decode()
    return f'<a href="data:text/plain;base64,{b64}" download="{filename}" style="display:inline-block;padding:8px 16px;background-color:#007bff;color:white;text-decoration:none;border-radius:4px;margin:5px 0;">ä¸‹è½½ {filename}</a>'

def main():
    st.title("å¤šæ–‡ä»¶åŸºå‡†åˆè§„æ€§åˆ†æå·¥å…·")
    st.write("ä¸Šä¼ ä¸€ä¸ªåŸºå‡†æ–‡ä»¶å’Œå¤šä¸ªç›®æ ‡æ–‡ä»¶ï¼Œç³»ç»Ÿå°†åˆ†ææ‰€æœ‰ç›®æ ‡æ–‡ä»¶ä¸åŸºå‡†æ–‡ä»¶çš„æ¡æ¬¾åˆè§„æ€§")
    
    # ä¾§è¾¹æ è®¾ç½®
    with st.sidebar:
        st.subheader("åˆ†æè®¾ç½®")
        api_key = st.text_input("Qwen APIå¯†é’¥", type="password")
        max_clauses = st.slider("æ¯ä¸ªæ–‡ä»¶æœ€å¤§åˆ†ææ¡æ¬¾æ•°", 5, 50, 20)
        st.info("æ¡æ¬¾æ•°é‡è¶Šå°‘ï¼Œåˆ†æé€Ÿåº¦è¶Šå¿«ï¼ŒæˆåŠŸç‡è¶Šé«˜")
    
    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    st.subheader("1. ä¸Šä¼ åŸºå‡†æ–‡ä»¶")
    base_file = st.file_uploader("ä¸Šä¼ åŸºå‡†PDFæ–‡ä»¶ï¼ˆä½œä¸ºåˆè§„æ€§æ ‡å‡†ï¼‰", type="pdf", key="base_file")
    
    st.subheader("2. ä¸Šä¼ ç›®æ ‡æ–‡ä»¶")
    target_files = st.file_uploader(
        "ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªéœ€è¦æ£€æŸ¥çš„PDFæ–‡ä»¶", 
        type="pdf", 
        key="target_files",
        accept_multiple_files=True
    )
    
    # åˆ†ææ§åˆ¶
    if st.button("å¼€å§‹åˆè§„æ€§åˆ†æ", disabled=not (base_file and target_files and api_key)):
        try:
            # å¤„ç†åŸºå‡†æ–‡ä»¶
            with st.spinner("æ­£åœ¨å¤„ç†åŸºå‡†æ–‡ä»¶..."):
                base_text = extract_text_from_pdf(base_file)
                if not base_text:
                    st.error("æ— æ³•ä»åŸºå‡†æ–‡ä»¶ä¸­æå–æ–‡æœ¬")
                    return
                
                base_clauses = split_into_clauses(base_text, max_clauses)
                st.success(f"åŸºå‡†æ–‡ä»¶å¤„ç†å®Œæˆ: {base_file.name} æå–åˆ° {len(base_clauses)} æ¡æ¡æ¬¾")
            
            # å‡†å¤‡å­˜å‚¨æ‰€æœ‰æŠ¥å‘Š
            all_reports = {}
            total_targets = len(target_files)
            
            # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
            global_progress_bar = st.progress(0)
            
            # å¤„ç†æ¯ä¸ªç›®æ ‡æ–‡ä»¶
            for target_idx, target_file in enumerate(target_files, 1):
                st.subheader(f"æ­£åœ¨åˆ†æç›®æ ‡æ–‡ä»¶ {target_idx}/{total_targets}: {target_file.name}")
                
                # æå–ç›®æ ‡æ–‡ä»¶æ–‡æœ¬å’Œæ¡æ¬¾
                with st.spinner(f"æå– {target_file.name} çš„æ¡æ¬¾..."):
                    target_text = extract_text_from_pdf(target_file)
                    if not target_text:
                        st.warning(f"æ— æ³•ä» {target_file.name} ä¸­æå–æ–‡æœ¬ï¼Œè·³è¿‡è¯¥æ–‡ä»¶")
                        continue
                    
                    target_clauses = split_into_clauses(target_text, max_clauses)
                    st.info(f"{target_file.name} æå–åˆ° {len(target_clauses)} æ¡æ¡æ¬¾")
                
                # åŒ¹é…æ¡æ¬¾
                with st.spinner(f"åŒ¹é… {target_file.name} ä¸åŸºå‡†æ–‡ä»¶çš„æ¡æ¬¾..."):
                    matched_pairs = match_clauses_with_base(base_clauses, target_clauses)
                    
                    if not matched_pairs:
                        st.warning(f"{target_file.name} æœªæ‰¾åˆ°ä¸åŸºå‡†æ–‡ä»¶åŒ¹é…çš„æ¡æ¬¾ï¼Œæ— æ³•åˆ†æ")
                        continue
                    
                    st.info(f"æ‰¾åˆ° {len(matched_pairs)} å¯¹å¯å¯¹æ¯”çš„æ¡æ¬¾")
                
                # ç”Ÿæˆåˆ†ææŠ¥å‘Š
                report = generate_target_report(
                    matched_pairs,
                    base_file.name,
                    target_file.name,
                    api_key,
                    target_idx - 1,  # 0-based index
                    total_targets
                )
                
                all_reports[target_file.name] = report
                
                # æ˜¾ç¤ºå•ä¸ªæ–‡ä»¶åˆ†æç»“æœ
                st.success(f"{target_file.name} åˆ†æå®Œæˆï¼")
                st.markdown(get_download_link(report, f"{target_file.name}_vs_{base_file.name}_åˆè§„æ€§æŠ¥å‘Š.txt"), unsafe_allow_html=True)
                
                with st.expander(f"æŸ¥çœ‹ {target_file.name} çš„åˆ†ææŠ¥å‘Šé¢„è§ˆ"):
                    st.text_area("æŠ¥å‘Šå†…å®¹", report, height=300)
                
                # æ›´æ–°æ€»ä½“è¿›åº¦
                global_progress = target_idx / total_targets
                global_progress_bar.progress(global_progress)
            
            # ç”Ÿæˆç»¼åˆæ‘˜è¦ï¼ˆå¦‚æœæœ‰å¤šä¸ªç›®æ ‡æ–‡ä»¶ï¼‰
            if len(all_reports) > 1:
                with st.spinner("ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶çš„ç»¼åˆåˆè§„æ€§æ‘˜è¦..."):
                    combined_summary = generate_combined_summary(all_reports, base_file.name, api_key)
                    
                    if combined_summary:
                        st.subheader("ğŸ“‹ æ‰€æœ‰æ–‡ä»¶ç»¼åˆåˆè§„æ€§è¯„ä¼°")
                        st.text_area("ç»¼åˆè¯„ä¼°å†…å®¹", combined_summary, height=400)
                        summary_filename = f"æ‰€æœ‰æ–‡ä»¶ä¸{base_file.name}_ç»¼åˆè¯„ä¼°.txt"
                        st.markdown(get_download_link(combined_summary, summary_filename), unsafe_allow_html=True)
            
            # æœ€ç»ˆæç¤º
            st.balloons()
            st.success("æ‰€æœ‰æ–‡ä»¶åˆ†æå®Œæˆï¼")
                
        except Exception as e:
            st.error(f"åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")
            
            # æ˜¾ç¤ºå·²å®Œæˆçš„éƒ¨åˆ†ç»“æœ
            if st.session_state.partial_reports:
                st.warning("å·²å®Œæˆéƒ¨åˆ†åˆ†æç»“æœï¼š")
                for name, partial_report in st.session_state.partial_reports.items():
                    report_text = "\n".join(partial_report)
                    st.markdown(get_download_link(report_text, f"éƒ¨åˆ†_{name}_vs_{base_file.name}_åˆè§„æ€§æŠ¥å‘Š.txt"), unsafe_allow_html=True)

if __name__ == "__main__":
    main()
    
