import streamlit as st
from PyPDF2 import PdfReader
import re
import jieba
import time
import matplotlib.pyplot as plt
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import tempfile

# è®¾ç½®é¡µé¢é…ç½® - ä¼˜å…ˆä¿è¯åŠ è½½é€Ÿåº¦
st.set_page_config(
    page_title="Qwen PDFåˆè§„åˆ†æå·¥å…·",
    page_icon="ğŸ“„",
    layout="wide"
)

# ç¡®ä¿ä¸­æ–‡æ˜¾ç¤ºæ­£å¸¸
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
plt.rcParams["axes.unicode_minus"] = False

# è‡ªå®šä¹‰CSS - ç®€åŒ–æ ·å¼æé«˜æ¸²æŸ“é€Ÿåº¦
st.markdown("""
<style>
    .stApp { max-width: 1200px; margin: 0 auto; }
    .clause-box { border-left: 4px solid #ccc; padding: 10px 15px; margin: 10px 0; }
    .clause-box.conflict { border-color: #dc3545; background-color: #fff5f5; }
    .clause-box.consistent { border-color: #28a745; background-color: #f8fff8; }
    .analysis-result { padding: 10px; border-radius: 5px; margin: 10px 0; }
</style>
""", unsafe_allow_html=True)

# æ£€æŸ¥å¹¶æ˜¾ç¤ºä¾èµ–çŠ¶æ€
def check_dependencies():
    """æ£€æŸ¥å…³é”®ä¾èµ–æ˜¯å¦å®‰è£…æ­£ç¡®"""
    try:
        import rich
        rich_version = rich.__version__
        if not (rich_version >= "10.14.0" and rich_version < "14.0.0"):
            st.warning(f"æ£€æµ‹åˆ°ä¸å…¼å®¹çš„richç‰ˆæœ¬: {rich_version}ï¼Œå»ºè®®å®‰è£…13.7.0ç‰ˆæœ¬")
    except ImportError:
        st.warning("æœªæ£€æµ‹åˆ°richåº“ï¼Œè¯·å®‰è£…13.7.0ç‰ˆæœ¬")

# ç¼“å­˜Qwenæ¨¡å‹åŠ è½½ - æé«˜é‡å¤ä½¿ç”¨é€Ÿåº¦
@st.cache_resource
def load_qwen_model(model_name="Qwen/Qwen-7B-Chat"):
    """åŠ è½½Qwenæ¨¡å‹å’Œtokenizerï¼Œä½¿ç”¨ç¼“å­˜æé«˜æ•ˆç‡"""
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨GPU
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        
        # åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨åŠç²¾åº¦æé«˜é€Ÿåº¦å’Œå‡å°‘å†…å­˜å ç”¨
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto",
            trust_remote_code=True
        ).eval()
        
        # åˆ›å»ºæ–‡æœ¬ç”Ÿæˆç®¡é“
        generator = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device=0 if device == "cuda" else -1
        )
        
        return generator, tokenizer, device
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        st.info("è¯·ç¡®ä¿å·²å®‰è£…æ­£ç¡®çš„ä¾èµ–ï¼Œæˆ–å°è¯•ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ç‰ˆæœ¬")
        return None, None, None

# å¿«é€ŸPDFæ–‡æœ¬æå–
def extract_text_from_pdf(file):
    """é«˜æ•ˆæå–PDFæ–‡æœ¬å†…å®¹"""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(file.getvalue())
            tmp_file_path = tmp_file.name
        
        pdf_reader = PdfReader(tmp_file_path)
        text = ""
        for page in pdf_reader.pages:
            page_text = page.extract_text() or ""
            text += page_text + "\n"
        
        os.unlink(tmp_file_path)  # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        return text
    except Exception as e:
        st.error(f"PDFæå–å¤±è´¥: {str(e)}")
        return ""

# æ¡æ¬¾æå–ä¼˜åŒ–ç‰ˆ
def extract_clauses(text):
    """å¿«é€Ÿæå–æ¡æ¬¾ï¼Œå‡å°‘ä¸å¿…è¦çš„æ­£åˆ™åŒ¹é…"""
    if not text:
        return []
    
    # ç®€åŒ–çš„æ¡æ¬¾æ¨¡å¼åŒ¹é…ï¼Œæé«˜é€Ÿåº¦
    clause_patterns = [
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+æ¡)',
        r'(ç¬¬\d+æ¡)',
        r'(\d+\.\s?[^ã€‚ï¼Œ,ï¼›;]+)',
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s?[^ã€‚ï¼Œ,ï¼›;]+)'
    ]
    
    clauses = []
    current_title = ""
    current_content = ""
    
    # æŒ‰è¡Œå¤„ç†ï¼Œå‡å°‘å†…å­˜å ç”¨
    for line in text.split('\n'):
        line = line.strip()
        if not line:
            continue
            
        matched = False
        for pattern in clause_patterns:
            match = re.search(pattern, line)
            if match:
                if current_title:  # ä¿å­˜ä¸Šä¸€ä¸ªæ¡æ¬¾
                    clauses.append({
                        "title": current_title,
                        "content": current_content.strip()
                    })
                
                current_title = match.group(1)
                current_content = line.replace(current_title, "", 1).strip()
                matched = True
                break
        
        if not matched and current_title:
            current_content += "\n" + line
    
    # æ·»åŠ æœ€åä¸€ä¸ªæ¡æ¬¾
    if current_title and current_content:
        clauses.append({
            "title": current_title,
            "content": current_content.strip()
        })
    
    return clauses

# ç¼“å­˜æ¡æ¬¾åŒ¹é… - é¿å…é‡å¤è®¡ç®—
@st.cache_data
def match_clauses(benchmark_clauses, compare_clauses):
    """å¿«é€ŸåŒ¹é…åŸºå‡†æ¡æ¬¾å’Œå¯¹æ¯”æ¡æ¬¾"""
    benchmark_map = {clause["title"]: clause for clause in benchmark_clauses}
    compare_map = {clause["title"]: clause for clause in compare_clauses}
    
    # åªä¿ç•™åŒæ–¹éƒ½æœ‰çš„æ¡æ¬¾
    common_titles = set(benchmark_map.keys()) & set(compare_map.keys())
    
    matched = []
    for title in common_titles:
        matched.append({
            "title": title,
            "benchmark": benchmark_map[title]["content"],
            "compare": compare_map[title]["content"]
        })
    
    return matched

# ä½¿ç”¨Qwenè¿›è¡Œåˆè§„æ€§åˆ†æ
def analyze_compliance_with_qwen(generator, tokenizer, benchmark_text, compare_text, title):
    """åˆ©ç”¨Qwenæ¨¡å‹åˆ†ææ¡æ¬¾åˆè§„æ€§"""
    if not generator or not tokenizer:
        return "æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œåˆ†æ", False
    
    # æ„å»ºç®€æ´çš„æç¤ºè¯ï¼Œå‡å°‘æ¨¡å‹è®¡ç®—é‡
    prompt = f"""
    ä»»åŠ¡ï¼šåˆ†æä¸¤ä¸ªæ¡æ¬¾çš„åˆè§„æ€§ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨å†²çªã€‚
    åŸºå‡†æ¡æ¬¾ï¼š{benchmark_text[:500]}
    å¯¹æ¯”æ¡æ¬¾ï¼š{compare_text[:500]}
    
    è¯·å›ç­”ï¼š
    1. ä¸¤æ¡æ¬¾çš„æ ¸å¿ƒå†…å®¹æ˜¯å¦ä¸€è‡´ï¼Ÿ
    2. å¦‚æœå­˜åœ¨å·®å¼‚ï¼Œæ˜¯å¦æ„æˆåˆè§„æ€§å†²çªï¼Ÿ
    3. ç®€è¦è¯´æ˜ç†ç”±ï¼ˆä¸è¶…è¿‡200å­—ï¼‰
    """
    
    try:
        # ä¼˜åŒ–ç”Ÿæˆå‚æ•°ï¼Œæé«˜é€Ÿåº¦
        result = generator(
            prompt,
            max_length=500,
            temperature=0.3,  # é™ä½éšæœºæ€§ï¼Œæé«˜ç¨³å®šæ€§
            top_p=0.8,
            repetition_penalty=1.1,
            do_sample=True,
            num_return_sequences=1
        )
        
        analysis = result[0]['generated_text'].replace(prompt, '').strip()
        
        # åˆ¤æ–­æ˜¯å¦å­˜åœ¨å†²çª
        has_conflict = any(keyword in analysis for keyword in 
                          ["å†²çª", "ä¸ä¸€è‡´", "ä¸ç¬¦åˆ", "è¿èƒŒ", "çŸ›ç›¾"])
        
        return analysis, has_conflict
    except Exception as e:
        st.warning(f"æ¡æ¬¾ '{title}' åˆ†æå¤±è´¥: {str(e)}")
        return f"åˆ†æå‡ºé”™: {str(e)}", True

# ä¸»åº”ç”¨
def main():
    # æ£€æŸ¥ä¾èµ–
    check_dependencies()
    
    st.title("ğŸ“„ Qwen PDFåˆè§„æ€§åˆ†æå·¥å…·")
    st.markdown("åŸºäºQwenå¤§æ¨¡å‹çš„æ¡æ¬¾åˆè§„æ€§åˆ†æï¼Œå¿«é€Ÿç¨³å®š")
    
    # ä¾§è¾¹æ  - æ¨¡å‹è®¾ç½®
    with st.sidebar:
        st.subheader("æ¨¡å‹è®¾ç½®")
        model_size = st.radio("é€‰æ‹©æ¨¡å‹å¤§å°", ["7B (è¾ƒå¿«)", "14B (è¾ƒå‡†)"], index=0)
        model_name = "Qwen/Qwen-7B-Chat" if model_size == "7B (è¾ƒå¿«)" else "Qwen/Qwen-14B-Chat"
        
        st.subheader("åˆ†æè®¾ç½®")
        batch_size = st.slider("æ‰¹é‡åˆ†ææ¡æ¬¾æ•°", 1, 5, 2)
        
        st.info("é¦–æ¬¡ä½¿ç”¨ä¼šä¸‹è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´")
        
        # é¢„åŠ è½½æ¨¡å‹
        with st.spinner("åŠ è½½Qwenæ¨¡å‹..."):
            generator, tokenizer, device = load_qwen_model(model_name)
        
        st.success(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}")
    
    # ä¸»å†…å®¹åŒº
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("åŸºå‡†æ–‡ä»¶")
        benchmark_file = st.file_uploader("ä¸Šä¼ åŸºå‡†PDF", type="pdf", key="benchmark")
    
    with col2:
        st.subheader("å¯¹æ¯”æ–‡ä»¶")
        compare_file = st.file_uploader("ä¸Šä¼ å¯¹æ¯”PDF", type="pdf", key="compare")
    
    # åˆ†ææŒ‰é’®
    if st.button("å¼€å§‹åˆè§„æ€§åˆ†æ", disabled=not (benchmark_file and compare_file and generator)):
        with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶..."):
            # æå–æ–‡æœ¬
            benchmark_text = extract_text_from_pdf(benchmark_file)
            compare_text = extract_text_from_pdf(compare_file)
            
            if not benchmark_text or not compare_text:
                st.error("æ— æ³•æå–PDFæ–‡æœ¬å†…å®¹")
                return
            
            # æå–æ¡æ¬¾
            benchmark_clauses = extract_clauses(benchmark_text)
            compare_clauses = extract_clauses(compare_text)
            
            st.info(f"æå–å®Œæˆ - åŸºå‡†æ–‡ä»¶: {len(benchmark_clauses)} æ¡æ¡æ¬¾ï¼Œå¯¹æ¯”æ–‡ä»¶: {len(compare_clauses)} æ¡æ¡æ¬¾")
            
            # åŒ¹é…æ¡æ¬¾
            matched_clauses = match_clauses(benchmark_clauses, compare_clauses)
            
            if not matched_clauses:
                st.warning("æœªæ‰¾åˆ°åŒ¹é…çš„æ¡æ¬¾ï¼Œæ— æ³•è¿›è¡Œåˆè§„æ€§åˆ†æ")
                return
            
            st.success(f"æ‰¾åˆ° {len(matched_clauses)} æ¡åŒ¹é…æ¡æ¬¾ï¼Œå¼€å§‹åˆ†æ...")
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        st.subheader("åˆ†æç»“æœ")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total = len(matched_clauses)
        conflict_count = 0
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # æ‰¹é‡å¤„ç†æ¡æ¬¾ï¼Œæé«˜æ•ˆç‡
        results = []
        for i, clause in enumerate(matched_clauses):
            status_text.text(f"æ­£åœ¨åˆ†ææ¡æ¬¾ {i+1}/{total}: {clause['title']}")
            
            # ä½¿ç”¨Qwenåˆ†æ
            analysis, has_conflict = analyze_compliance_with_qwen(
                generator, 
                tokenizer,
                clause["benchmark"], 
                clause["compare"],
                clause["title"]
            )
            
            if has_conflict:
                conflict_count += 1
            
            results.append({
                "title": clause["title"],
                "benchmark": clause["benchmark"],
                "compare": clause["compare"],
                "analysis": analysis,
                "has_conflict": has_conflict
            })
            
            # æ›´æ–°è¿›åº¦
            progress_bar.progress((i + 1) / total)
        
        progress_bar.empty()
        status_text.empty()
        
        # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
        col1, col2 = st.columns(2)
        col1.metric("æ€»åŒ¹é…æ¡æ¬¾æ•°", total)
        col2.metric("å­˜åœ¨å†²çªçš„æ¡æ¬¾æ•°", conflict_count)
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        st.subheader("æ¡æ¬¾è¯¦ç»†åˆ†æ")
        
        # å…ˆæ˜¾ç¤ºå†²çªæ¡æ¬¾
        st.markdown("### âš ï¸ å­˜åœ¨å†²çªçš„æ¡æ¬¾")
        conflict_found = False
        for res in results:
            if res["has_conflict"]:
                conflict_found = True
                with st.expander(f"æ¡æ¬¾: {res['title']}", expanded=True):
                    st.markdown(f"<div class='clause-box'><strong>åŸºå‡†æ¡æ¬¾:</strong><br>{res['benchmark'][:300]}...</div>", unsafe_allow_html=True)
                    st.markdown(f"<div class='clause-box conflict'><strong>å¯¹æ¯”æ¡æ¬¾:</strong><br>{res['compare'][:300]}...</div>", unsafe_allow_html=True)
                    st.markdown(f"<div class='analysis-result'><strong>åˆè§„æ€§åˆ†æ:</strong><br>{res['analysis']}</div>", unsafe_allow_html=True)
        
        if not conflict_found:
            st.success("æœªå‘ç°å­˜åœ¨å†²çªçš„æ¡æ¬¾")
        
        # å†æ˜¾ç¤ºåˆè§„æ¡æ¬¾
        st.markdown("### âœ… åˆè§„çš„æ¡æ¬¾")
        for res in results:
            if not res["has_conflict"]:
                with st.expander(f"æ¡æ¬¾: {res['title']}", expanded=False):
                    st.markdown(f"<div class='clause-box'><strong>åŸºå‡†æ¡æ¬¾:</strong><br>{res['benchmark'][:300]}...</div>", unsafe_allow_html=True)
                    st.markdown(f"<div class='clause-box consistent'><strong>å¯¹æ¯”æ¡æ¬¾:</strong><br>{res['compare'][:300]}...</div>", unsafe_allow_html=True)
                    st.markdown(f"<div class='analysis-result'><strong>åˆè§„æ€§åˆ†æ:</strong><br>{res['analysis']}</div>", unsafe_allow_html=True)

if __name__ == "__main__":
    main()
