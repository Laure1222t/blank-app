import streamlit as st
from PyPDF2 import PdfReader
from difflib import SequenceMatcher
import base64
import re
import requests
import jieba
from typing import List, Tuple, Dict

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·ï¼ˆ1å¯¹å¤šï¼‰",
    page_icon="ğŸ“„",
    layout="wide"
)

# è‡ªå®šä¹‰æ ·å¼ï¼ˆå¢å¼ºè§†è§‰åŒºåˆ†åº¦ï¼‰
st.markdown("""
<style>
    .stApp { max-width: 1400px; margin: 0 auto; }
    .file-selector { border: 1px solid #e0e0e0; padding: 15px; border-radius: 5px; margin-bottom: 15px; }
    .benchmark-label { color: #0066cc; font-weight: bold; }
    .target-label { color: #cc6600; font-weight: bold; }
    .highlight-conflict { background-color: #ffeeba; padding: 2px 4px; border-radius: 3px; }
    .clause-box { border-left: 4px solid #007bff; padding: 10px; margin: 10px 0; background-color: #f8f9fa; }
    .compliance-ok { border-left: 4px solid #28a745; }
    .compliance-warning { border-left: 4px solid #ffc107; }
    .compliance-conflict { border-left: 4px solid #dc3545; }
    .model-response { background-color: #f0f2f6; padding: 15px; border-radius: 5px; margin: 10px 0; }
    .section-title { border-bottom: 2px solid #f0f2f6; padding-bottom: 8px; margin-top: 20px; }
</style>
""", unsafe_allow_html=True)

# APIé…ç½®
QWEN_API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"

def call_qwen_api(prompt: str, api_key: str) -> str:
    """è°ƒç”¨Qwenå¤§æ¨¡å‹API"""
    if not api_key:
        st.error("Qwen APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·åœ¨å·¦ä¾§æ è¾“å…¥å¯†é’¥")
        return None
        
    try:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        data = {
            "model": "qwen-plus",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 1500
        }
        
        response = requests.post(
            QWEN_API_URL,
            headers=headers,
            json=data,
            timeout=60
        )
        
        if response.status_code != 200:
            st.error(f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}ï¼Œå“åº”: {response.text}")
            return None
            
        response_json = response.json()
        if "choices" not in response_json or len(response_json["choices"]) == 0:
            st.error("APIè¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")
            return None
            
        return response_json["choices"][0]["message"]["content"]
        
    except requests.exceptions.Timeout:
        st.error("APIè¯·æ±‚è¶…æ—¶ï¼Œè¯·é‡è¯•")
        return None
    except Exception as e:
        st.error(f"è°ƒç”¨Qwen APIå¤±è´¥: {str(e)}")
        return None

def extract_text_from_pdf(file) -> str:
    """ä»PDFæå–æ–‡æœ¬ï¼ˆä¼˜åŒ–ä¸­æ–‡å¤„ç†ï¼‰"""
    try:
        pdf_reader = PdfReader(file)
        text = ""
        for page in pdf_reader.pages:
            page_text = page.extract_text() or ""
            # å¤„ç†ä¸­æ–‡æ’ç‰ˆé—®é¢˜
            page_text = page_text.replace("  ", "").replace("\n", "").replace("\r", "")
            text += page_text
        return text
    except Exception as e:
        st.error(f"æå–æ–‡æœ¬å¤±è´¥: {str(e)}")
        return ""

def split_into_clauses(text: str) -> List[str]:
    """å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ¡æ¬¾ï¼ˆå¢å¼ºä¸­æ–‡æ¡æ¬¾è¯†åˆ«ï¼‰"""
    patterns = [
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡\s+.*?)(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡\s+|$)',
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s+.*?)(?=[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s+|$)',
        r'(\d+\.\s+.*?)(?=\d+\.\s+|$)',
        r'(\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s+.*?)(?=\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s+|$)',
        r'(\([1-9]+\)\s+.*?)(?=\([1-9]+\)\s+|$)',
        r'(ã€[^\ã€‘]+ã€‘\s+.*?)(?=ã€[^\ã€‘]+ã€‘\s+|$)'
    ]
    
    for pattern in patterns:
        clauses = re.findall(pattern, text, re.DOTALL)
        if len(clauses) > 3:
            return [clause.strip() for clause in clauses if clause.strip()]
    
    # å…œåº•åˆ†å‰²æ–¹æ¡ˆ
    paragraphs = re.split(r'[ã€‚ï¼›ï¼ï¼Ÿ]\s*', text)
    return [p.strip() for p in paragraphs if p.strip() and len(p) > 10]

def chinese_text_similarity(text1: str, text2: str) -> float:
    """è®¡ç®—ä¸­æ–‡æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆåˆ†è¯ä¼˜åŒ–ï¼‰"""
    words1 = list(jieba.cut(text1))
    words2 = list(jieba.cut(text2))
    return SequenceMatcher(None, words1, words2).ratio()

def match_clauses(benchmark_clauses: List[str], target_clauses: List[str]) -> Tuple[List, List, List]:
    """åŒ¹é…åŸºå‡†æ¡æ¬¾ä¸ç›®æ ‡æ¡æ¬¾"""
    matched_pairs = []
    used_indices = set()
    
    for i, bench_clause in enumerate(benchmark_clauses):
        best_match = None
        best_ratio = 0.25  # ä¸­æ–‡åŒ¹é…é˜ˆå€¼
        best_j = -1
        
        for j, target_clause in enumerate(target_clauses):
            if j not in used_indices:
                ratio = chinese_text_similarity(bench_clause, target_clause)
                if ratio > best_ratio:
                    best_ratio = ratio
                    best_match = target_clause
                    best_j = j
        
        if best_match:
            matched_pairs.append((bench_clause, best_match, best_ratio))
            used_indices.add(best_j)
    
    unmatched_bench = [clause for i, clause in enumerate(benchmark_clauses) 
                     if i not in [idx for idx, _ in enumerate(matched_pairs)]]
    unmatched_target = [clause for j, clause in enumerate(target_clauses) if j not in used_indices]
    
    return matched_pairs, unmatched_bench, unmatched_target

def create_download_link(content: str, filename: str, text: str) -> str:
    """ç”Ÿæˆä¸‹è½½é“¾æ¥"""
    b64 = base64.b64encode(content.encode()).decode()
    return f'<a href="data:file/txt;base64,{b64}" download="{filename}">{text}</a>'

def analyze_compliance_with_qwen(bench_clause: str, target_clause: str, 
                                bench_name: str, target_name: str, api_key: str) -> str:
    """ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ¡æ¬¾åˆè§„æ€§"""
    prompt = f"""
    è¯·åˆ†æä»¥ä¸‹ä¸¤ä¸ªæ¡æ¬¾çš„åˆè§„æ€§ï¼ˆä»¥{bench_name}ä¸ºåŸºå‡†ï¼‰ï¼š
    
    {bench_name}æ¡æ¬¾ï¼š{bench_clause}
    {target_name}æ¡æ¬¾ï¼š{target_clause}
    
    æŒ‰ä»¥ä¸‹ç»“æ„å›ç­”ï¼š
    1. ç›¸ä¼¼åº¦è¯„ä¼°ï¼šé«˜/ä¸­/ä½åŠç†ç”±
    2. å·®å¼‚ç‚¹åˆ†æï¼šè¡¨è¿°ã€èŒƒå›´ã€è¦æ±‚çš„å…·ä½“å·®å¼‚
    3. åˆè§„æ€§åˆ¤æ–­ï¼šæ— å†²çª/è½»å¾®å†²çª/ä¸¥é‡å†²çª
    4. å†²çªåŸå› ï¼ˆå¦‚å­˜åœ¨ï¼‰ï¼šå…·ä½“åŸå› åŠæ½œåœ¨å½±å“
    5. å»ºè®®ï¼šä¸“ä¸šå¤„ç†å»ºè®®
    
    æ³¨æ„ä¸­æ–‡æ³•å¾‹æœ¯è¯­å·®å¼‚ï¼ˆå¦‚"åº”å½“"ä¸"å¿…é¡»"ã€"ä¸å¾—"ä¸"ç¦æ­¢"ï¼‰
    """
    return call_qwen_api(prompt, api_key)

def analyze_standalone_clause_with_qwen(clause: str, doc_name: str, api_key: str) -> str:
    """åˆ†æç‹¬ç«‹æ¡æ¬¾"""
    prompt = f"""
    åˆ†æä»¥ä¸‹æ¡æ¬¾ï¼ˆæ¥è‡ª{doc_name}ï¼‰ï¼š{clause}
    è¯·è¯„ä¼°ï¼šä¸»è¦å†…å®¹ã€æ ¸å¿ƒè¦æ±‚ã€æ½œåœ¨å½±å“åŠå¯èƒ½å­˜åœ¨çš„é—®é¢˜ï¼Œç»™å‡ºå»ºè®®ã€‚
    """
    return call_qwen_api(prompt, api_key)

def analyze_single_target(bench_text: str, target_text: str, 
                         bench_name: str, target_name: str, api_key: str) -> Dict:
    """åˆ†æå•ä¸ªç›®æ ‡æ–‡ä»¶ä¸åŸºå‡†æ–‡ä»¶çš„å¯¹æ¯”ç»“æœ"""
    # æ¡æ¬¾åˆ†å‰²
    bench_clauses = split_into_clauses(bench_text)
    target_clauses = split_into_clauses(target_text)
    
    # æ¡æ¬¾åŒ¹é…
    matched_pairs, unmatched_bench, unmatched_target = match_clauses(bench_clauses, target_clauses)
    
    # ç”Ÿæˆåˆ†æç»“æœ
    analysis_results = {
        "bench_name": bench_name,
        "target_name": target_name,
        "bench_count": len(bench_clauses),
        "target_count": len(target_clauses),
        "matched_count": len(matched_pairs),
        "matched_pairs": matched_pairs,
        "unmatched_bench": unmatched_bench,
        "unmatched_target": unmatched_target,
        "compliance_analyses": []
    }
    
    # åˆè§„æ€§åˆ†æ
    for bench_clause, target_clause, ratio in matched_pairs:
        analysis = analyze_compliance_with_qwen(
            bench_clause, target_clause, bench_name, target_name, api_key
        )
        analysis_results["compliance_analyses"].append(analysis)
    
    return analysis_results

def show_multi_target_analysis(bench_text: str, target_files: List, 
                              bench_name: str, api_key: str):
    """æ˜¾ç¤ºå¤šç›®æ ‡æ–‡ä»¶åˆ†æç»“æœ"""
    # åŸºå‡†æ¡æ¬¾é¢„å¤„ç†
    bench_clauses = split_into_clauses(bench_text)
    st.success(f"åŸºå‡†æ–‡ä»¶æ¡æ¬¾è§£æå®Œæˆï¼š{bench_name} è¯†åˆ«å‡º {len(bench_clauses)} æ¡æ¡æ¬¾")

    # ç›®æ ‡æ–‡ä»¶é€‰æ‹©å™¨
    st.subheader("ğŸ” é€‰æ‹©ç›®æ ‡æ–‡ä»¶è¿›è¡Œå¯¹æ¯”")
    selected_targets = st.multiselect(
        "å·²ä¸Šä¼ ç›®æ ‡æ–‡ä»¶",
        options=[f.name for f in target_files],
        default=[f.name for f in target_files[:2]]  # é»˜è®¤é€‰æ‹©å‰2ä¸ª
    )

    if not selected_targets:
        st.info("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç›®æ ‡æ–‡ä»¶")
        return

    # æ‰¹é‡åˆ†æ
    for target_name in selected_targets:
        # æ‰¾åˆ°å¯¹åº”çš„æ–‡ä»¶å¯¹è±¡
        target_file = next(f for f in target_files if f.name == target_name)
        target_text = extract_text_from_pdf(target_file)
        
        if not target_text:
            st.error(f"æ— æ³•æå– {target_name} çš„æ–‡æœ¬å†…å®¹")
            continue

        # æ˜¾ç¤ºå•ä¸ªç›®æ ‡åˆ†æç»“æœ
        st.divider()
        st.header(f"ğŸ“Œ {target_name} ä¸ {bench_name} å¯¹æ¯”åˆ†æ")
        
        # æ‰§è¡Œåˆ†æ
        with st.spinner(f"æ­£åœ¨åˆ†æ {target_name}..."):
            result = analyze_single_target(
                bench_text, target_text, bench_name, target_name, api_key
            )
        
        # ç»Ÿè®¡ä¿¡æ¯
        col1, col2, col3 = st.columns(3)
        col1.metric("åŸºå‡†æ¡æ¬¾æ•°", result["bench_count"])
        col2.metric(f"{target_name} æ¡æ¬¾æ•°", result["target_count"])
        col3.metric("åŒ¹é…æ¡æ¬¾æ•°", result["matched_count"])

        # è¯¦ç»†åˆ†æ
        st.subheader("æ¡æ¬¾åŒ¹é…åŠåˆè§„æ€§åˆ†æ")
        for i in range(result["matched_count"]):
            st.markdown(f"### åŒ¹é…å¯¹ {i+1}ï¼ˆç›¸ä¼¼åº¦: {result['matched_pairs'][i][2]:.2%}ï¼‰")
            
            # æ¡æ¬¾å¯¹æ¯”å±•ç¤º
            col_a, col_b = st.columns(2)
            with col_a:
                st.markdown(f'<div class="clause-box compliance-ok"><strong>{bench_name} æ¡æ¬¾:</strong><br>{result["matched_pairs"][i][0]}</div>', unsafe_allow_html=True)
            with col_b:
                st.markdown(f'<div class="clause-box"><strong>{target_name} æ¡æ¬¾:</strong><br>{result["matched_pairs"][i][1]}</div>', unsafe_allow_html=True)
            
            # åˆè§„æ€§åˆ†æç»“æœ
            if result["compliance_analyses"][i]:
                st.markdown(
                    f'<div class="model-response"><strong>åˆè§„æ€§åˆ†æ:</strong><br>{result["compliance_analyses"][i]}</div>',
                    unsafe_allow_html=True
                )
            st.divider()

        # æœªåŒ¹é…æ¡æ¬¾
        st.subheader("æœªåŒ¹é…æ¡æ¬¾åˆ†æ")
        col_un1, col_un2 = st.columns(2)
        with col_un1:
            st.markdown(f"#### {bench_name} ç‹¬æœ‰çš„æ¡æ¬¾ï¼ˆ{len(result['unmatched_bench'])}ï¼‰")
            for i, clause in enumerate(result["unmatched_bench"][:5]):  # æ˜¾ç¤ºå‰5æ¡
                st.markdown(f'<div class="clause-box"><strong>æ¡æ¬¾ {i+1}:</strong><br>{clause}</div>', unsafe_allow_html=True)
                if i >= 4:
                    st.text(f"... å…± {len(result['unmatched_bench'])} æ¡ï¼ˆä»…æ˜¾ç¤ºå‰5æ¡ï¼‰")
                    break

        with col_un2:
            st.markdown(f"#### {target_name} ç‹¬æœ‰çš„æ¡æ¬¾ï¼ˆ{len(result['unmatched_target'])}ï¼‰")
            for i, clause in enumerate(result["unmatched_target"][:5]):
                st.markdown(f'<div class="clause-box"><strong>æ¡æ¬¾ {i+1}:</strong><br>{clause}</div>', unsafe_allow_html=True)
                if i >= 4:
                    st.text(f"... å…± {len(result['unmatched_target'])} æ¡ï¼ˆä»…æ˜¾ç¤ºå‰5æ¡ï¼‰")
                    break

# ä¸»ç•Œé¢
def main():
    st.title("ğŸ“„ Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·ï¼ˆ1å¯¹å¤šï¼‰")
    st.markdown("æ”¯æŒ1ä¸ªåŸºå‡†æ–‡ä»¶ä¸å¤šä¸ªç›®æ ‡æ–‡ä»¶çš„æ¡æ¬¾åˆè§„æ€§æ¯”å¯¹")

    # ä¾§è¾¹æ APIè®¾ç½®
    with st.sidebar:
        st.subheader("Qwen API è®¾ç½®")
        qwen_api_key = st.text_input("è¯·è¾“å…¥Qwen APIå¯†é’¥", type="password")
        st.markdown(f"å½“å‰APIç«¯ç‚¹ï¼š`{QWEN_API_URL}`")
        st.divider()
        st.info("ä½¿ç”¨è¯´æ˜ï¼š\n1. ä¸Šä¼ 1ä¸ªåŸºå‡†æ–‡ä»¶\n2. ä¸Šä¼ å¤šä¸ªç›®æ ‡æ–‡ä»¶\n3. é€‰æ‹©ç›®æ ‡æ–‡ä»¶è¿›è¡Œå¯¹æ¯”")

    # æ–‡ä»¶ä¸Šä¼ åŒºï¼ˆå‚è€ƒå›¾ç‰‡å¸ƒå±€ï¼‰
    st.subheader("ğŸ“‚ æ–‡ä»¶ä¸Šä¼ åŒº")
    
    # åŸºå‡†æ–‡ä»¶ä¸Šä¼ 
    st.markdown('<div class="file-selector"><span class="benchmark-label">åŸºå‡†æ–‡ä»¶ï¼ˆå¿…å¡«ï¼‰ï¼š</span>é€‰æ‹©ä½œä¸ºåˆè§„æ€§åˆ¤æ–­ä¾æ®çš„æ–‡ä»¶</div>', unsafe_allow_html=True)
    bench_file = st.file_uploader(
        "ä¸Šä¼ åŸºå‡†æ–‡ä»¶ï¼ˆä»…æ”¯æŒPDFï¼‰",
        type=["pdf"],
        key="benchmark",
        accept_multiple_files=False
    )

    # ç›®æ ‡æ–‡ä»¶ä¸Šä¼ 
    st.markdown('<div class="file-selector"><span class="target-label">ç›®æ ‡æ–‡ä»¶ï¼ˆå¯å¤šä¸ªï¼‰ï¼š</span>éœ€è¦è¿›è¡Œåˆè§„æ€§æ£€æŸ¥çš„æ–‡ä»¶</div>', unsafe_allow_html=True)
    target_files = st.file_uploader(
        "ä¸Šä¼ ç›®æ ‡æ–‡ä»¶ï¼ˆä»…æ”¯æŒPDFï¼‰",
        type=["pdf"],
        key="targets",
        accept_multiple_files=True
    )

    # åˆ†ææŒ‰é’®
    if st.button("å¼€å§‹1å¯¹å¤šåˆè§„æ€§åˆ†æ", type="primary"):
        if not bench_file:
            st.error("è¯·å…ˆä¸Šä¼ åŸºå‡†æ–‡ä»¶")
            return
        if not target_files:
            st.error("è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªç›®æ ‡æ–‡ä»¶")
            return

        # åŸºå‡†æ–‡ä»¶å¤„ç†
        with st.spinner("æ­£åœ¨è§£æåŸºå‡†æ–‡ä»¶..."):
            bench_text = extract_text_from_pdf(bench_file)
            if not bench_text:
                st.error("æ— æ³•æå–åŸºå‡†æ–‡ä»¶æ–‡æœ¬ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æœ‰æ•ˆæ€§")
                return

        # æ˜¾ç¤ºåˆ†æç»“æœ
        show_multi_target_analysis(bench_text, target_files, bench_file.name, qwen_api_key)

    # é¡µè„š
    st.divider()
    st.markdown("""
    <div style="text-align:center; color:#666; margin-top:20px;">
        ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·ï¼ˆ1å¯¹å¤šç‰ˆï¼‰ | åŸºäºQwenå¤§æ¨¡å‹
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
