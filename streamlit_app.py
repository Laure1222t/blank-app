import streamlit as st
from PyPDF2 import PdfReader
from difflib import SequenceMatcher
import base64
import re
import requests
import jieba
import hashlib
import time
import io
from functools import lru_cache
from collections import defaultdict
# æ–°å¢OCRç›¸å…³åº“
from pdf2image import convert_from_path
import pytesseract
from PIL import Image

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡
st.set_page_config(
    page_title="Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·",
    page_icon="ğŸ“„",
    layout="wide"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .stApp { max-width: 1200px; margin: 0 auto; }
    .stFileUploader { width: 100%; }
    .highlight-conflict { background-color: #ffeeba; padding: 2px 4px; border-radius: 3px; }
    .clause-box { border-left: 4px solid #007bff; padding: 10px; margin: 10px 0; background-color: #f8f9fa; }
    .compliance-ok { border-left: 4px solid #28a745; }
    .compliance-warning { border-left: 4px solid #ffc107; }
    .compliance-conflict { border-left: 4px solid #dc3545; }
    .model-response { background-color: #f0f2f6; padding: 15px; border-radius: 5px; margin: 10px 0; }
    .processing-bar { background-color: #e9ecef; border-radius: 5px; padding: 3px; margin: 10px 0; }
    .processing-progress { background-color: #007bff; height: 10px; border-radius: 3px; }
    .section-header { background-color: #f8f9fa; padding: 10px; border-radius: 5px; margin: 15px 0; }
</style>
""", unsafe_allow_html=True)

# é…ç½®Qwen APIå‚æ•°
QWEN_API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"

# ç¼“å­˜ç®¡ç†
cache = defaultdict(dict)

def get_cache_key(*args):
    """ç”Ÿæˆç¼“å­˜é”®"""
    return hashlib.md5(str(args).encode()).hexdigest()

def cached_func(func):
    """å‡½æ•°ç¼“å­˜è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        key = get_cache_key(args, kwargs)
        if key in cache[func.__name__]:
            return cache[func.__name__][key]
        result = func(*args, **kwargs)
        cache[func.__name__][key] = result
        return result
    return wrapper

@cached_func
def call_qwen_api(prompt, api_key, retry=3):
    """è°ƒç”¨Qwenå¤§æ¨¡å‹APIï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    if not api_key:
        st.error("Qwen APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·åœ¨å·¦ä¾§æ è¾“å…¥å¯†é’¥")
        return None
        
    try:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        data = {
            "model": "qwen-plus",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 800  # å‡å°‘æœ€å¤§ tokensï¼Œä½¿å›ç­”æ›´ç®€æ´
        }
        
        # å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨
        for attempt in range(retry):
            try:
                response = requests.post(
                    QWEN_API_URL,
                    headers=headers,
                    json=data,
                    timeout=60
                )
                
                if response.status_code == 200:
                    response_json = response.json()
                    if "choices" in response_json and len(response_json["choices"]) > 0:
                        return response_json["choices"][0]["message"]["content"]
                    else:
                        st.warning(f"APIè¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ (å°è¯• {attempt+1}/{retry})")
                else:
                    st.warning(f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code} (å°è¯• {attempt+1}/{retry})")
                
                time.sleep(2** attempt)  # æŒ‡æ•°é€€é¿
                
            except requests.exceptions.Timeout:
                st.warning(f"APIè¯·æ±‚è¶…æ—¶ (å°è¯• {attempt+1}/{retry})")
                time.sleep(2 **attempt)
            except Exception as e:
                st.warning(f"APIè°ƒç”¨å¼‚å¸¸: {str(e)} (å°è¯• {attempt+1}/{retry})")
                time.sleep(2** attempt)
                
        st.error("APIè°ƒç”¨å¤šæ¬¡å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")
        return None
        
    except Exception as e:
        st.error(f"è°ƒç”¨Qwen APIå¤±è´¥: {str(e)}")
        return None

# æ–°å¢OCRç›¸å…³å‡½æ•°
def ocr_image(image):
    """å¯¹å•å¼ å›¾ç‰‡è¿›è¡ŒOCRè¯†åˆ«ï¼Œæå–ä¸­æ–‡æ–‡æœ¬"""
    try:
        # é…ç½®Tesseractè¯†åˆ«ä¸­æ–‡
        custom_config = r'--oem 3 --psm 6 -l chi_sim'
        text = pytesseract.image_to_string(image, config=custom_config)
        return text
    except Exception as e:
        st.warning(f"OCRè¯†åˆ«å‡ºé”™: {str(e)}")
        return ""

def extract_text_from_image_pdf(file, progress_bar=None):
    """ä»å›¾ç‰‡PDFä¸­æå–æ–‡æœ¬ï¼ˆå…ˆè½¬ä¸ºå›¾ç‰‡å†è¿›è¡ŒOCRï¼‰"""
    try:
        # å°†PDFä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_path = f"temp_{hashlib.md5(file.read()).hexdigest()}.pdf"
        file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        with open(temp_path, "wb") as f:
            f.write(file.read())
        
        # å°†PDFè½¬æ¢ä¸ºå›¾ç‰‡
        pages = convert_from_path(temp_path, 300)  # 300 DPIæé«˜è¯†åˆ«ç²¾åº¦
        total_pages = len(pages)
        text = ""
        
        for i, page in enumerate(pages):
            # å¯¹æ¯ä¸€é¡µè¿›è¡ŒOCR
            page_text = ocr_image(page)
            # å¤„ç†è¯†åˆ«ç»“æœ
            page_text = page_text.replace("  ", "").replace("\n", "").replace("\r", "")
            text += page_text
            
            # æ›´æ–°è¿›åº¦æ¡
            if progress_bar is not None:
                progress = (i + 1) / total_pages
                progress_bar.progress(progress)
                progress_bar.text(f"OCRå¤„ç†: ç¬¬ {i+1}/{total_pages} é¡µ")
        
        return text
    except Exception as e:
        st.error(f"å›¾ç‰‡PDFå¤„ç†å¤±è´¥: {str(e)}")
        return ""

def is_image_based_pdf(file):
    """åˆ¤æ–­PDFæ˜¯å¦ä¸ºå›¾ç‰‡å‹PDFï¼ˆæ— æ–‡æœ¬å±‚ï¼‰"""
    try:
        # å°è¯•æå–æ–‡æœ¬
        pdf_reader = PdfReader(file)
        file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        
        # æ£€æŸ¥å‰å‡ é¡µæ˜¯å¦æœ‰æ–‡æœ¬
        sample_text = ""
        for i, page in enumerate(pdf_reader.pages):
            if i >= 3:  # æ£€æŸ¥å‰3é¡µ
                break
            sample_text += page.extract_text() or ""
            
        # å¦‚æœæå–çš„æ–‡æœ¬å¾ˆå°‘ï¼Œè§†ä¸ºå›¾ç‰‡å‹PDF
        return len(sample_text.strip()) < 50
    except Exception as e:
        st.warning(f"PDFç±»å‹æ£€æµ‹å‡ºé”™: {str(e)}")
        return False

def extract_text_from_pdf(file, progress_bar=None):
    """ä»PDFæå–æ–‡æœ¬ï¼Œæ”¯æŒæ™®é€šPDFå’Œå›¾ç‰‡PDFï¼ˆé€šè¿‡OCRï¼‰"""
    try:
        # å…ˆåˆ¤æ–­PDFç±»å‹
        file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        if is_image_based_pdf(file):
            st.info("æ£€æµ‹åˆ°å›¾ç‰‡å‹PDFï¼Œå°†ä½¿ç”¨OCRè¿›è¡Œæ–‡å­—è¯†åˆ«ï¼ˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰")
            file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            return extract_text_from_image_pdf(file, progress_bar)
        
        # æ™®é€šPDFæ–‡æœ¬æå–
        file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        pdf_reader = PdfReader(file)
        text = ""
        total_pages = len(pdf_reader.pages)
        
        for i, page in enumerate(pdf_reader.pages):
            page_text = page.extract_text() or ""
            # å¤„ç†ä¸­æ–‡ç©ºæ ¼å’Œæ¢è¡Œé—®é¢˜
            page_text = page_text.replace("  ", "").replace("\n", "").replace("\r", "")
            text += page_text
            
            # æ›´æ–°è¿›åº¦æ¡
            if progress_bar is not None:
                progress = (i + 1) / total_pages
                progress_bar.progress(progress)
                progress_bar.text(f"æå–æ–‡æœ¬: ç¬¬ {i+1}/{total_pages} é¡µ")
        
        return text
    except Exception as e:
        st.error(f"æå–æ–‡æœ¬å¤±è´¥: {str(e)}")
        return ""

def split_into_clauses(text, doc_name="æ–‡æ¡£"):
    """å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ¡æ¬¾ï¼Œå¢å¼ºä¸­æ–‡æ¡æ¬¾è¯†åˆ«å’Œå¤§æ–‡æ¡£å¤„ç†"""
    # å¢å¼ºä¸­æ–‡æ¡æ¬¾æ¨¡å¼è¯†åˆ«ï¼Œæ›´å…¨é¢çš„æ¨¡å¼åº“
    patterns = [
        # ä¸»è¦æ¡æ¬¾æ¨¡å¼
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡(?:ä¹‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)?\s*[:ï¼š]?\s*.*?)(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡(?:ä¹‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)?\s*[:ï¼š]?\s*|$)',
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ã€\s*.*?)(?=[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ã€\s*|$)',
        r'(\d+\.\s*.*?)(?=\d+\.\s*|$)',
        r'(\(\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*\)\s*.*?)(?=\(\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*\)\s*|$)',
        r'(\(\s*[1-9]+\d*\s*\)\s*.*?)(?=\(\s*[1-9]+\d*\s*\)\s*|$)',
        r'([ï¼¡-ï¼ºï½-ï½š]\.\s*.*?)(?=[ï¼¡-ï¼ºï½-ï½š]\.\s*|$)',
        r'(ã€[^ã€‘]+ã€‘\s*.*?)(?=ã€[^ã€‘]+ã€‘\s*|$)',
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¬¾\s*.*?)(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¬¾\s*|$)',
    ]
    
    # å°è¯•å„ç§æ¨¡å¼ï¼Œæ‰¾åˆ°æœ€ä½³åˆ†å‰²
    best_clauses = []
    for pattern in patterns:
        clauses = re.findall(pattern, text, re.DOTALL)
        # è¿‡æ»¤è¿‡çŸ­æ¡æ¬¾
        clauses = [clause.strip() for clause in clauses if clause.strip() and len(clause.strip()) > 10]
        if len(clauses) > len(best_clauses) and len(clauses) > 2:
            best_clauses = clauses
    
    # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿçš„æ¡æ¬¾ï¼Œè¿”å›ç»“æœ
    if best_clauses:
        return best_clauses
    
    # å°è¯•æ®µè½åˆ†å‰²ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
    paragraphs = re.split(r'[ã€‚ï¼›ï¼ï¼Ÿ]\s*', text)
    paragraphs = [p.strip() for p in paragraphs if p.strip() and len(p) > 10]
    
    # å¦‚æœæ®µè½æ•°é‡ä»ç„¶å¾ˆå°‘ï¼Œå°è¯•æŒ‰å›ºå®šé•¿åº¦åˆ†å—ï¼ˆå¤„ç†éå¸¸å¤§çš„æ–‡æ¡£ï¼‰
    if len(paragraphs) < 3 and len(text) > 5000:
        chunk_size = 1000  # æ¯ä¸ªå—å¤§çº¦1000å­—ç¬¦
        paragraphs = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
        st.warning(f"{doc_name} æ¡æ¬¾ç»“æ„ä¸æ˜æ˜¾ï¼Œå·²æŒ‰ {chunk_size} å­—ç¬¦é•¿åº¦åˆ†å—å¤„ç†")
    
    return paragraphs

@lru_cache(maxsize=1000)
def chinese_text_similarity(text1, text2):
    """è®¡ç®—ä¸­æ–‡æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œä½¿ç”¨åˆ†è¯ååŒ¹é…ï¼Œç»“æœç¼“å­˜"""
    # è¿‡æ»¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
    text1_clean = re.sub(r'[^\w\s]', '', text1)
    text2_clean = re.sub(r'[^\w\s]', '', text2)
    
    # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
    words1 = list(jieba.cut(text1_clean))
    words2 = list(jieba.cut(text2_clean))
    
    # è®¡ç®—åˆ†è¯åçš„ç›¸ä¼¼åº¦
    return SequenceMatcher(None, words1, words2).ratio()

def extract_key_terms(text):
    """æå–æ–‡æœ¬ä¸­çš„å…³é”®æœ¯è¯­ï¼Œç”¨äºå¢å¼ºåŒ¹é…"""
    # ç®€å•å®ç°ï¼šæå–å¯èƒ½çš„æ¡æ¬¾å·å’Œå…³é”®åè¯
    terms = set()
    
    # æå–æ¡æ¬¾å·
    clause_numbers = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡', text)
    terms.update(clause_numbers)
    
    # æå–å¯èƒ½çš„å…³é”®åè¯ï¼ˆç®€å•å¤„ç†ï¼‰
    nouns = re.findall(r'ã€[^ã€‘]+ã€‘', text)
    terms.update(nouns)
    
    return terms

def match_clauses(clauses1, clauses2, progress_container=None):
    """åŒ¹é…ä¸¤ä¸ªæ–‡æ¡£ä¸­çš„ç›¸ä¼¼æ¡æ¬¾ï¼Œä¼˜åŒ–ä¸­æ–‡åŒ¹é…å’Œå¤§æ–‡æ¡£å¤„ç†"""
    # é¢„å…ˆè®¡ç®—æ‰€æœ‰æ¡æ¬¾çš„å…³é”®æœ¯è¯­
    terms1 = [extract_key_terms(clause) for clause in clauses1]
    terms2 = [extract_key_terms(clause) for clause in clauses2]
    
    # å…ˆåŸºäºå…³é”®æœ¯è¯­è¿›è¡Œåˆæ­¥åŒ¹é…
    term_matches = defaultdict(list)
    for i, terms in enumerate(terms1):
        if terms:
            for j, other_terms in enumerate(terms2):
                overlap = len(terms & other_terms)
                if overlap > 0:
                    term_matches[i].append((j, overlap))
    
    matched_pairs = []
    used_indices = set()
    total = len(clauses1)
    
    for i, clause1 in enumerate(clauses1):
        # æ›´æ–°è¿›åº¦
        if progress_container is not None:
            progress = (i + 1) / total
            with progress_container:
                st.progress(progress)
                st.text(f"åŒ¹é…æ¡æ¬¾: {i+1}/{total}")
        
        best_match = None
        best_ratio = 0.25  # åŸºç¡€é˜ˆå€¼
        best_j = -1
        
        # ä¼˜å…ˆè€ƒè™‘æœ‰å…³é”®æœ¯è¯­åŒ¹é…çš„æ¡æ¬¾
        candidates = []
        if i in term_matches:
            # æŒ‰æœ¯è¯­é‡å åº¦æ’åº
            for j, _ in sorted(term_matches[i], key=lambda x: x[1], reverse=True):
                if j not in used_indices:
                    candidates.append(j)
        
        # å¦‚æœæ²¡æœ‰æœ¯è¯­åŒ¹é…ï¼Œè€ƒè™‘æ‰€æœ‰æœªåŒ¹é…çš„æ¡æ¬¾
        if not candidates:
            candidates = [j for j in range(len(clauses2)) if j not in used_indices]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        for j in candidates:
            ratio = chinese_text_similarity(clause1, clauses2[j])
            
            # å¦‚æœæœ‰å…³é”®æœ¯è¯­åŒ¹é…ï¼Œé€‚å½“æé«˜ç›¸ä¼¼åº¦åˆ†æ•°
            if i in term_matches and any(j == k for k, _ in term_matches[i]):
                ratio = min(1.0, ratio * 1.1)  # æé«˜10%
                
            if ratio > best_ratio:
                best_ratio = ratio
                best_match = clauses2[j]
                best_j = j
        
        if best_match:
            matched_pairs.append((clause1, best_match, best_ratio))
            used_indices.add(best_j)
    
    # å¤„ç†æœªåŒ¹é…çš„æ¡æ¬¾
    unmatched1 = [clause for i, clause in enumerate(clauses1) 
                 if i not in [idx for idx, _ in enumerate(matched_pairs)]]
    unmatched2 = [clause for j, clause in enumerate(clauses2) if j not in used_indices]
    
    return matched_pairs, unmatched1, unmatched2

def create_download_link(content, filename, text):
    """ç”Ÿæˆä¸‹è½½é“¾æ¥"""
    b64 = base64.b64encode(content.encode()).decode()
    return f'<a href="data:file/txt;base64,{b64}" download="{filename}">{text}</a>'

def analyze_compliance_with_qwen(clause1, clause2, filename1, filename2, api_key):
    """ä½¿ç”¨Qwenå¤§æ¨¡å‹åˆ†ææ¡æ¬¾åˆè§„æ€§ï¼Œç®€åŒ–å·®å¼‚ç‚¹åˆ†æ"""
    prompt = f"""
    è¯·ç®€è¦åˆ†æä»¥ä¸‹ä¸¤ä¸ªä¸­æ–‡æ¡æ¬¾çš„åˆè§„æ€§ï¼Œåˆ¤æ–­å®ƒä»¬æ˜¯å¦å­˜åœ¨å†²çªï¼š
    
    {filename1} æ¡æ¬¾ï¼š{clause1}
    
    {filename2} æ¡æ¬¾ï¼š{clause2}
    
    è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”¨ä¸­æ–‡è¿›è¡Œç®€æ´åˆ†æï¼ˆæ€»å­—æ•°æ§åˆ¶åœ¨300å­—ä»¥å†…ï¼‰ï¼š
    1. ç›¸ä¼¼åº¦è¯„ä¼°ï¼šç®€è¦è¯´æ˜ç›¸ä¼¼ç¨‹åº¦ï¼ˆé«˜/ä¸­/ä½ï¼‰
    2. ä¸»è¦å·®å¼‚ï¼šåˆ—å‡º1-2ä¸ªæœ€æ ¸å¿ƒçš„å·®å¼‚ç‚¹
    3. åˆè§„æ€§åˆ¤æ–­ï¼šæ˜¯å¦å­˜åœ¨å†²çªï¼ˆæ— å†²çª/è½»å¾®å†²çª/ä¸¥é‡å†²çªï¼‰
    4. ç®€è¦å»ºè®®ï¼šé’ˆå¯¹å‘ç°çš„é—®é¢˜ï¼Œç»™å‡ºç®€çŸ­å»ºè®®
    
    åˆ†æè¯·ç®€æ˜æ‰¼è¦ï¼Œé¿å…å†—é•¿æè¿°ã€‚
    """
    
    return call_qwen_api(prompt, api_key)

def analyze_standalone_clause_with_qwen(clause, doc_name, api_key):
    """ä½¿ç”¨Qwenå¤§æ¨¡å‹åˆ†æç‹¬ç«‹æ¡æ¬¾ï¼ˆæœªåŒ¹é…çš„æ¡æ¬¾ï¼‰ï¼Œç»“æœæ›´ç®€æ´"""
    prompt = f"""
    è¯·ç®€è¦åˆ†æä»¥ä¸‹ä¸­æ–‡æ¡æ¬¾çš„å†…å®¹ï¼ˆæ€»å­—æ•°æ§åˆ¶åœ¨200å­—ä»¥å†…ï¼‰ï¼š
    
    {doc_name} ä¸­çš„æ¡æ¬¾ï¼š{clause}
    
    è¯·ç”¨ä¸­æ–‡ç®€è¦è¯´æ˜è¯¥æ¡æ¬¾çš„æ ¸å¿ƒå†…å®¹å’Œä¸»è¦è¦æ±‚ï¼Œæ— éœ€å±•å¼€è¯¦ç»†åˆ†æã€‚
    """
    
    return call_qwen_api(prompt, api_key)

def analyze_document_structure(text, doc_name, api_key):
    """åˆ†ææ–‡æ¡£ç»“æ„ï¼Œè·å–æ–‡æ¡£æ¦‚è¿°å’Œä¸»è¦ç« èŠ‚ï¼Œç»“æœæ›´ç®€æ´"""
    if not api_key:
        return None
        
    prompt = f"""
    è¯·ç®€è¦åˆ†æä»¥ä¸‹æ–‡æ¡£çš„ç»“æ„å¹¶æä¾›æ¦‚è¿°ï¼ˆæ€»å­—æ•°æ§åˆ¶åœ¨200å­—ä»¥å†…ï¼‰ï¼š
    
    æ–‡æ¡£åç§°ï¼š{doc_name}
    æ–‡æ¡£å†…å®¹ï¼š{text[:3000]}  # åªå–å‰3000å­—ç¬¦è¿›è¡Œåˆ†æ
    
    è¯·ç®€æ˜æä¾›ï¼š
    1. æ–‡æ¡£ç±»å‹å’Œä¸»é¢˜æ¦‚è¿°
    2. ä¸»è¦ç« èŠ‚æˆ–æ¡æ¬¾åˆ†ç±»ï¼ˆæœ€å¤š5é¡¹ï¼‰
    3. æ–‡æ¡£çš„æ ¸å¿ƒç›®çš„
    
    åˆ†æåº”éå¸¸ç®€æ´ï¼Œé¿å…ç»†èŠ‚æè¿°ã€‚
    """
    
    return call_qwen_api(prompt, api_key)

def chunk_large_document(text, chunk_size=5000, overlap=500):
    """å°†å¤§æ–‡æ¡£åˆ†å—å¤„ç†"""
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        # ä¸‹ä¸€å—ä¸å½“å‰å—é‡å ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§
        start = end - overlap
        
        if start >= text_length:
            break
            
    return chunks

def analyze_single_comparison(base_clauses, compare_text, base_name, compare_name, api_key):
    """åˆ†æå•ä¸ªå¯¹æ¯”æ–‡ä»¶ä¸åŸºå‡†æ–‡ä»¶çš„åˆè§„æ€§ï¼Œæ”¯æŒå¤§æ–‡æ¡£å¤„ç†"""
    # æ£€æŸ¥æ–‡æ¡£å¤§å°ï¼Œå†³å®šæ˜¯å¦åˆ†å—å¤„ç†
    if len(compare_text) > 10000:  # è¶…è¿‡10000å­—ç¬¦çš„æ–‡æ¡£è§†ä¸ºå¤§æ–‡æ¡£
        st.info(f"{compare_name} æ˜¯ä¸€ä¸ªå¤§æ–‡æ¡£ï¼ˆ{len(compare_text)}å­—ç¬¦ï¼‰ï¼Œå°†è¿›è¡Œåˆ†å—å¤„ç†")
        chunks = chunk_large_document(compare_text)
        st.info(f"æ–‡æ¡£å·²åˆ†ä¸º {len(chunks)} ä¸ªå¤„ç†å—")
        
        all_compare_clauses = []
        for i, chunk in enumerate(chunks):
            with st.expander(f"å¤„ç†å— {i+1}/{len(chunks)}", expanded=False):
                chunk_clauses = split_into_clauses(chunk, f"{compare_name} (å— {i+1})")
                st.success(f"å— {i+1} è¯†åˆ«å‡º {len(chunk_clauses)} æ¡æ¡æ¬¾")
                all_compare_clauses.extend(chunk_clauses)
        
        compare_clauses = all_compare_clauses
    else:
        # åˆ†å‰²å¯¹æ¯”æ–‡ä»¶æ¡æ¬¾
        with st.spinner(f"æ­£åœ¨åˆ†æ {compare_name} çš„æ¡æ¬¾ç»“æ„..."):
            compare_clauses = split_into_clauses(compare_text, compare_name)
            st.success(f"{compare_name} æ¡æ¬¾åˆ†æå®Œæˆï¼Œè¯†åˆ«å‡º {len(compare_clauses)} æ¡æ¡æ¬¾")
    
    # åŒ¹é…æ¡æ¬¾ï¼Œæ˜¾ç¤ºè¿›åº¦
    progress_container = st.empty()
    with st.spinner(f"æ­£åœ¨åŒ¹é… {base_name} ä¸ {compare_name} çš„ç›¸ä¼¼æ¡æ¬¾..."):
        matched_pairs, unmatched_base, unmatched_compare = match_clauses(
            base_clauses, 
            compare_clauses,
            progress_container
        )
    progress_container.empty()
    
    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
    st.divider()
    col1, col2, col3, col4 = st.columns(4)
    col1.metric(f"{base_name} æ¡æ¬¾æ•°", len(base_clauses))
    col2.metric(f"{compare_name} æ¡æ¬¾æ•°", len(compare_clauses))
    col3.metric("åŒ¹é…æ¡æ¬¾æ•°", len(matched_pairs))
    col4.metric("æœªåŒ¹é…æ¡æ¬¾æ•°", len(unmatched_base) + len(unmatched_compare))
    
    # æ˜¾ç¤ºæ¡æ¬¾å¯¹æ¯”å’Œåˆè§„æ€§åˆ†æ
    st.divider()
    st.subheader(f"ğŸ“Š {compare_name} ä¸ {base_name} æ¡æ¬¾åˆè§„æ€§åˆ†æï¼ˆQwenå¤§æ¨¡å‹ï¼‰")
    
    # åˆ›å»ºåˆ†æç»“æœçš„æ ‡ç­¾é¡µå¯¼èˆª
    tab_labels = ["å…¨éƒ¨åŒ¹é…é¡¹"]
    if len(unmatched_base) > 0:
        tab_labels.append(f"{base_name} ç‹¬æœ‰æ¡æ¬¾")
    if len(unmatched_compare) > 0:
        tab_labels.append(f"{compare_name} ç‹¬æœ‰æ¡æ¬¾")
    
    tabs = st.tabs(tab_labels)
    tab_idx = 0
    
    # åˆ†ææ¯ä¸ªåŒ¹é…å¯¹çš„åˆè§„æ€§
    with tabs[tab_idx]:
        tab_idx += 1
        
        # æ·»åŠ ç­›é€‰åŠŸèƒ½
        min_similarity = st.slider("æœ€ä½ç›¸ä¼¼åº¦ç­›é€‰", 0.0, 1.0, 0.0, 0.05)
        filtered_pairs = [p for p in matched_pairs if p[2] >= min_similarity]
        
        st.write(f"æ˜¾ç¤º {len(filtered_pairs)} ä¸ªåŒ¹é…é¡¹ï¼ˆç­›é€‰åï¼‰")
        
        for i, (clause1, clause2, ratio) in enumerate(filtered_pairs):
            # æ ¹æ®ç›¸ä¼¼åº¦è®¾ç½®ä¸åŒé¢œè‰²æ ‡è¯†
            if ratio > 0.7:
                similarity_color = "#28a745"  # ç»¿è‰² - é«˜ç›¸ä¼¼åº¦
                similarity_label = "é«˜ç›¸ä¼¼åº¦"
            elif ratio > 0.4:
                similarity_color = "#ffc107"  # é»„è‰² - ä¸­ç›¸ä¼¼åº¦
                similarity_label = "ä¸­ç›¸ä¼¼åº¦"
            else:
                similarity_color = "#dc3545"  # çº¢è‰² - ä½ç›¸ä¼¼åº¦
                similarity_label = "ä½ç›¸ä¼¼åº¦"
            
            st.markdown(f"### åŒ¹é…æ¡æ¬¾å¯¹ {i+1}")
            st.markdown(f'<span style="color:{similarity_color};font-weight:bold">{similarity_label}: {ratio:.2%}</span>', unsafe_allow_html=True)
            
            col_a, col_b = st.columns(2)
            with col_a:
                st.markdown(f'<div class="clause-box"><strong>{base_name} æ¡æ¬¾:</strong><br>{clause1}</div>', unsafe_allow_html=True)
            with col_b:
                st.markdown(f'<div class="clause-box"><strong>{compare_name} æ¡æ¬¾:</strong><br>{clause2}</div>', unsafe_allow_html=True)
            
            # æ·»åŠ åˆ†æç»“æœæŠ˜å æ¡†
            with st.expander("æŸ¥çœ‹Qwenå¤§æ¨¡å‹åˆ†æ", expanded=False):
                with st.spinner("æ­£åœ¨è°ƒç”¨Qwenå¤§æ¨¡å‹è¿›è¡Œåˆ†æ..."):
                    analysis = analyze_compliance_with_qwen(clause1, clause2, base_name, compare_name, api_key)
                
                if analysis:
                    st.markdown('<div class="model-response"><strong>Qwenåˆ†æç»“æœ:</strong><br>' + analysis + '</div>', unsafe_allow_html=True)
                else:
                    st.warning("æœªèƒ½è·å–åˆ†æç»“æœ")
            
            st.divider()
    
    # æœªåŒ¹é…çš„æ¡æ¬¾åˆ†æ - åŸºå‡†æ–‡ä»¶ç‹¬æœ‰
    if len(unmatched_base) > 0 and tab_idx < len(tabs):
        with tabs[tab_idx]:
            tab_idx += 1
            st.markdown(f"#### {base_name} ä¸­ç‹¬æœ‰çš„æ¡æ¬¾ ({len(unmatched_base)})")
            
            # å…è®¸ç”¨æˆ·é€‰æ‹©æŸ¥çœ‹ç‰¹å®šæ¡æ¬¾
            selected_clause = st.selectbox(
                "é€‰æ‹©è¦æŸ¥çœ‹çš„æ¡æ¬¾",
                range(len(unmatched_base)),
                format_func=lambda x: f"æ¡æ¬¾ {x+1}ï¼ˆ{min(50, len(unmatched_base[x]))}å­—ï¼‰"
            )
            
            clause = unmatched_base[selected_clause]
            st.markdown(f'<div class="clause-box"><strong>æ¡æ¬¾ {selected_clause+1}:</strong><br>{clause}</div>', unsafe_allow_html=True)
            
            with st.spinner("Qwenå¤§æ¨¡å‹æ­£åœ¨åˆ†ææ­¤æ¡æ¬¾..."):
                analysis = analyze_standalone_clause_with_qwen(clause, base_name, api_key)
            
            if analysis:
                st.markdown('<div class="model-response"><strong>Qwenåˆ†æ:</strong><br>' + analysis + '</div>', unsafe_allow_html=True)
    
    # æœªåŒ¹é…çš„æ¡æ¬¾åˆ†æ - å¯¹æ¯”æ–‡ä»¶ç‹¬æœ‰
    if len(unmatched_compare) > 0 and tab_idx < len(tabs):
        with tabs[tab_idx]:
            tab_idx += 1
            st.markdown(f"#### {compare_name} ä¸­ç‹¬æœ‰çš„æ¡æ¬¾ ({len(unmatched_compare)})")
            
            # å…è®¸ç”¨æˆ·é€‰æ‹©æŸ¥çœ‹ç‰¹å®šæ¡æ¬¾
            selected_clause = st.selectbox(
                "é€‰æ‹©è¦æŸ¥çœ‹çš„æ¡æ¬¾",
                range(len(unmatched_compare)),
                format_func=lambda x: f"æ¡æ¬¾ {x+1}ï¼ˆ{min(50, len(unmatched_compare[x]))}å­—ï¼‰"
            )
            
            clause = unmatched_compare[selected_clause]
            st.markdown(f'<div class="clause-box"><strong>æ¡æ¬¾ {selected_clause+1}:</strong><br>{clause}</div>', unsafe_allow_html=True)
            
            with st.spinner("Qwenå¤§æ¨¡å‹æ­£åœ¨åˆ†ææ­¤æ¡æ¬¾..."):
                analysis = analyze_standalone_clause_with_qwen(clause, compare_name, api_key)
            
            if analysis:
                st.markdown('<div class="model-response"><strong>Qwenåˆ†æ:</strong><br>' + analysis + '</div>', unsafe_allow_html=True)

# åº”ç”¨ä¸»ç•Œé¢
st.title("ğŸ“„ Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·")
st.markdown("ä¸“ä¸ºä¸­æ–‡æ–‡æ¡£ä¼˜åŒ–çš„æ™ºèƒ½æ¡æ¬¾åˆè§„æ€§åˆ†æç³»ç»Ÿ - æ”¯æŒå¤§æ–‡æ¡£ã€å›¾ç‰‡PDFå’Œä¸€å¯¹å¤šåˆ†æ")

# æ–°å¢OCRé…ç½®è¯´æ˜
with st.expander("ğŸ“Œ å…³äºå›¾ç‰‡PDFå¤„ç†", expanded=False):
    st.markdown("""
    æœ¬å·¥å…·æ”¯æŒå¤„ç†å›¾ç‰‡è½¬PDFä¸­çš„æ–‡å­—ï¼ˆé€šè¿‡OCRæŠ€æœ¯ï¼‰ï¼Œä½†éœ€è¦é¢å¤–é…ç½®ï¼š
    
    1. å®‰è£…Tesseract OCRå¼•æ“ï¼š
       - Windows: ä¸‹è½½å®‰è£… [Tesseract OCR](https://github.com/UB-Mannheim/tesseract/wiki)
       - macOS: `brew install tesseract tesseract-lang`
       - Linux: `sudo apt install tesseract-ocr tesseract-ocr-chi-sim`
    
    2. ç¡®ä¿ä¸­æ–‡è¯­è¨€åŒ…å·²å®‰è£…ï¼ˆç”¨äºè¯†åˆ«ä¸­æ–‡æ–‡æœ¬ï¼‰
    """)

# Qwen APIè®¾ç½®
with st.sidebar:
    st.subheader("Qwen API è®¾ç½®")
    qwen_api_key = st.text_input("è¯·è¾“å…¥Qwen APIå¯†é’¥", type="password")
    st.markdown(f"""
    æç¤ºï¼šAPIå¯†é’¥å¯ä»¥ä»é˜¿é‡Œäº‘DashScopeæ§åˆ¶å°è·å–ã€‚
    å½“å‰ä½¿ç”¨çš„APIç«¯ç‚¹ï¼š`{QWEN_API_URL}`
    """)
    
    # OCRè®¾ç½®
    st.subheader("OCR è®¾ç½®")
    tesseract_path = st.text_input(
        "Tesseract OCRå®‰è£…è·¯å¾„ï¼ˆå¯é€‰ï¼‰", 
        value=r"C:\Program Files\Tesseract-OCR\tesseract.exe" if st.runtime.platform == "windows" else "/usr/bin/tesseract"
    )
    # é…ç½®Tesseractè·¯å¾„
    pytesseract.pytesseract.tesseract_cmd = tesseract_path
    
    # é«˜çº§è®¾ç½®
    with st.expander("é«˜çº§è®¾ç½®", expanded=False):
        similarity_threshold = st.slider("æ¡æ¬¾åŒ¹é…ç›¸ä¼¼åº¦é˜ˆå€¼", 0.0, 1.0, 0.25, 0.05)
        max_api_retries = st.slider("APIæœ€å¤§é‡è¯•æ¬¡æ•°", 1, 5, 3)
        chunk_size = st.slider("å¤§æ–‡æ¡£åˆ†å—å¤§å°ï¼ˆå­—ç¬¦ï¼‰", 2000, 10000, 5000, 500)
        ocr_dpi = st.slider("OCRè¯†åˆ«ç²¾åº¦ï¼ˆDPIï¼‰", 150, 600, 300, 50)

with st.form("upload_form"):
    st.subheader("åŸºå‡†æ–‡ä»¶")
    base_file = st.file_uploader("é€‰æ‹©åŸºå‡†PDFæ–‡ä»¶", type=["pdf"], key="base_file")
    
    st.subheader("å¯¹æ¯”æ–‡ä»¶ï¼ˆå¯ä¸Šä¼ å¤šä¸ªï¼‰")
    compare_files = st.file_uploader(
        "é€‰æ‹©éœ€è¦å¯¹æ¯”çš„PDFæ–‡ä»¶", 
        type=["pdf"], 
        key="compare_files",
        accept_multiple_files=True
    )
    
    # åˆ†æé€‰é¡¹
    with st.expander("åˆ†æé€‰é¡¹", expanded=False):
        analyze_structure = st.checkbox("åˆ†ææ–‡æ¡£ç»“æ„å¹¶ç”Ÿæˆæ¦‚è¿°", value=True)
        show_all_matches = st.checkbox("æ˜¾ç¤ºæ‰€æœ‰åŒ¹é…é¡¹ï¼ˆåŒ…æ‹¬ä½ç›¸ä¼¼åº¦ï¼‰", value=True)
        detailed_analysis = st.checkbox("ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š", value=False)  # é»˜è®¤ä¸ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        force_ocr = st.checkbox("å¯¹æ‰€æœ‰PDFå¼ºåˆ¶ä½¿ç”¨OCRï¼ˆå³ä½¿æœ‰æ–‡æœ¬å±‚ï¼‰", value=False)
    
    submitted = st.form_submit_button("å¼€å§‹åˆè§„æ€§åˆ†æ")

if submitted and base_file and compare_files:
    if not qwen_api_key:
        st.warning("æœªæ£€æµ‹åˆ°Qwen APIå¯†é’¥ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™")
    
    # åˆ›å»ºæ€»ä½“è¿›åº¦è·Ÿè¸ª
    overall_progress = st.progress(0)
    total_steps = 1 + len(compare_files) * 3  # åŸºå‡†æ–‡ä»¶å¤„ç† + æ¯ä¸ªå¯¹æ¯”æ–‡ä»¶çš„3ä¸ªæ­¥éª¤
    current_step = 0
    
    with st.spinner("æ­£åœ¨è§£æåŸºå‡†PDFå†…å®¹ï¼Œè¯·ç¨å€™..."):
        # æ˜¾ç¤ºåŸºå‡†æ–‡ä»¶å¤„ç†è¿›åº¦
        progress_bar = st.progress(0)
        
        # æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶OCR
        if force_ocr:
            base_text = extract_text_from_image_pdf(base_file, progress_bar)
        else:
            base_text = extract_text_from_pdf(base_file, progress_bar)
            
        progress_bar.empty()
        
        current_step += 1
        overall_progress.progress(current_step / total_steps)
        
        if not base_text:
            st.error("æ— æ³•æå–åŸºå‡†æ–‡ä»¶çš„æ–‡æœ¬å†…å®¹ï¼Œè¯·ç¡®è®¤PDFåŒ…å«å¯æå–çš„ä¸­æ–‡æ–‡æœ¬")
        else:
            st.success(f"åŸºå‡†æ–‡ä»¶ {base_file.name} æ–‡æœ¬æå–å®Œæˆï¼ˆ{len(base_text)}å­—ç¬¦ï¼‰")
            
            # åˆ†æåŸºå‡†æ–‡æ¡£ç»“æ„
            if analyze_structure and qwen_api_key:
                with st.spinner("æ­£åœ¨åˆ†æåŸºå‡†æ–‡æ¡£ç»“æ„..."):
                    base_structure = analyze_document_structure(base_text, base_file.name, qwen_api_key)
                    if base_structure:
                        st.markdown('<div class="section-header"><strong>åŸºå‡†æ–‡æ¡£ç»“æ„åˆ†æ:</strong></div>', unsafe_allow_html=True)
                        st.markdown('<div class="model-response">' + base_structure + '</div>', unsafe_allow_html=True)
            
            # å¤„ç†å¤§åŸºå‡†æ–‡æ¡£
            if len(base_text) > 10000:
                st.info(f"åŸºå‡†æ–‡ä»¶ {base_file.name} æ˜¯ä¸€ä¸ªå¤§æ–‡æ¡£ï¼ˆ{len(base_text)}å­—ç¬¦ï¼‰ï¼Œå°†è¿›è¡Œåˆ†å—å¤„ç†")
                chunks = chunk_large_document(base_text, chunk_size)
                st.info(f"åŸºå‡†æ–‡æ¡£å·²åˆ†ä¸º {len(chunks)} ä¸ªå¤„ç†å—")
                
                base_clauses = []
                for i, chunk in enumerate(chunks):
                    with st.expander(f"åŸºå‡†æ–‡æ¡£å¤„ç†å— {i+1}/{len(chunks)}", expanded=False):
                        chunk_clauses = split_into_clauses(chunk, f"{base_file.name} (å— {i+1})")
                        st.success(f"å— {i+1} è¯†åˆ«å‡º {len(chunk_clauses)} æ¡æ¡æ¬¾")
                        base_clauses.extend(chunk_clauses)
            else:
                # é¢„å¤„ç†åŸºå‡†æ–‡ä»¶æ¡æ¬¾
                base_clauses = split_into_clauses(base_text, base_file.name)
            
            st.success(f"åŸºå‡†æ–‡ä»¶ {base_file.name} æ¡æ¬¾è§£æå®Œæˆï¼Œå…±è¯†åˆ«å‡º {len(base_clauses)} æ¡æ¡æ¬¾")
            current_step += 1
            overall_progress.progress(current_step / total_steps)
            
            # å¯¹æ¯ä¸ªå¯¹æ¯”æ–‡ä»¶è¿›è¡Œåˆ†æ
            for i, compare_file in enumerate(compare_files, 1):
                st.markdown(f"## ğŸ” åˆ†æ {i}/{len(compare_files)}: {compare_file.name} ä¸ {base_file.name} çš„å¯¹æ¯”")
                
                # æå–å¯¹æ¯”æ–‡ä»¶æ–‡æœ¬
                with st.spinner(f"æ­£åœ¨æå– {compare_file.name} çš„æ–‡æœ¬å†…å®¹..."):
                    progress_bar = st.progress(0)
                    
                    if force_ocr:
                        compare_text = extract_text_from_image_pdf(compare_file, progress_bar)
                    else:
                        compare_text = extract_text_from_pdf(compare_file, progress_bar)
                        
                    progress_bar.empty()
                
                current_step += 1
                overall_progress.progress(current_step / total_steps)
                
                if not compare_text:
                    st.error(f"æ— æ³•æå– {compare_file.name} çš„æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡è¯¥æ–‡ä»¶")
                    continue
                
                # åˆ†æå¯¹æ¯”æ–‡æ¡£ç»“æ„
                if analyze_structure and qwen_api_key:
                    with st.spinner(f"æ­£åœ¨åˆ†æ {compare_file.name} çš„æ–‡æ¡£ç»“æ„..."):
                        compare_structure = analyze_document_structure(compare_text, compare_file.name, qwen_api_key)
                        if compare_structure:
                            st.markdown(f'<div class="section-header"><strong>{compare_file.name} ç»“æ„åˆ†æ:</strong></div>', unsafe_allow_html=True)
                            st.markdown('<div class="model-response">' + compare_structure + '</div>', unsafe_allow_html=True)
                
                current_step += 1
                overall_progress.progress(current_step / total_steps)
                
                # åˆ†æå½“å‰å¯¹æ¯”æ–‡ä»¶ä¸åŸºå‡†æ–‡ä»¶
                analyze_single_comparison(
                    base_clauses, 
                    compare_text, 
                    base_file.name, 
                    compare_file.name, 
                    qwen_api_key
                )
                
                current_step += 1
                overall_progress.progress(current_step / total_steps)
                
                # åœ¨æ–‡ä»¶åˆ†æä¹‹é—´æ·»åŠ åˆ†éš”
                st.markdown("---")
        
        # å®Œæˆæ‰€æœ‰åˆ†æ
        overall_progress.empty()
        st.success("æ‰€æœ‰æ–‡æ¡£åˆ†æå·²å®Œæˆï¼")
        
        # æä¾›æ•´ä½“åˆ†ææŠ¥å‘Šä¸‹è½½ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if detailed_analysis and qwen_api_key:
            with st.spinner("æ­£åœ¨ç”Ÿæˆæ•´ä½“åˆ†ææŠ¥å‘Š..."):
                report_prompt = f"""
                åŸºäºä¹‹å‰å¯¹åŸºå‡†æ–‡ä»¶ {base_file.name} å’Œå¯¹æ¯”æ–‡ä»¶ {[f.name for f in compare_files]} çš„åˆ†æï¼Œ
                è¯·ç”Ÿæˆä¸€ä»½ç®€æ´çš„ç»¼åˆåˆè§„æ€§åˆ†ææŠ¥å‘Šï¼ˆæ§åˆ¶åœ¨500å­—ä»¥å†…ï¼‰ï¼ŒåŒ…æ‹¬ï¼š
                1. æ•´ä½“åˆè§„æ€§è¯„ä¼°
                2. ä¸»è¦å†²çªç‚¹æ±‡æ€»ï¼ˆæœ€å¤š3é¡¹ï¼‰
                3. ç®€è¦æ”¹è¿›å»ºè®®
                
                æŠ¥å‘Šåº”éå¸¸ç®€æ´ï¼Œé‡ç‚¹çªå‡ºã€‚
                """
                report = call_qwen_api(report_prompt, qwen_api_key)
                
                if report:
                    st.markdown('<div class="section-header"><strong>æ•´ä½“åˆè§„æ€§åˆ†ææŠ¥å‘Š:</strong></div>', unsafe_allow_html=True)
                    st.markdown(report)
                    
                    # åˆ›å»ºä¸‹è½½é“¾æ¥
                    report_content = f"Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†ææŠ¥å‘Š\n\nåŸºå‡†æ–‡ä»¶: {base_file.name}\nå¯¹æ¯”æ–‡ä»¶: {', '.join([f.name for f in compare_files])}\n\n{report}"
                    download_link = create_download_link(report_content, "compliance_report.txt", "ä¸‹è½½åˆ†ææŠ¥å‘Š")
                    st.markdown(download_link, unsafe_allow_html=True)
elif submitted:
    if not base_file:
        st.error("è¯·ä¸Šä¼ åŸºå‡†PDFæ–‡ä»¶")
    if not compare_files:
        st.error("è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªå¯¹æ¯”PDFæ–‡ä»¶")
else:
    st.info('è¯·ä¸Šä¼ ä¸€ä¸ªåŸºå‡†PDFæ–‡ä»¶å’Œè‡³å°‘ä¸€ä¸ªå¯¹æ¯”PDFæ–‡ä»¶ï¼Œç„¶åç‚¹å‡»"å¼€å§‹åˆè§„æ€§åˆ†æ"æŒ‰é’®')

# æ·»åŠ é¡µè„š
st.divider()
st.markdown("""
<style>
.footer {
    font-size: 0.8rem;
    color: #666;
    text-align: center;
    margin-top: 2rem;
}
</style>
<div class="footer">
    ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…· | åŸºäºQwenå¤§æ¨¡å‹ | æ”¯æŒå›¾ç‰‡PDFè¯†åˆ« | ä¼˜åŒ–ä¸­æ–‡æ–‡æ¡£å¤„ç†
</div>
""", unsafe_allow_html=True)
