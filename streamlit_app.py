import streamlit as st
from PyPDF2 import PdfReader
from difflib import SequenceMatcher
import base64
import re
import requests
import jieba
import hashlib
import time
from functools import lru_cache
from collections import defaultdict

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡
st.set_page_config(
    page_title="Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·",
    page_icon="ğŸ“„",
    layout="wide"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .stApp { max-width: 1200px; margin: 0 auto; }
    .stFileUploader { width: 100%; }
    .highlight-conflict { background-color: #ffeeba; padding: 2px 4px; border-radius: 3px; }
    .clause-box { border-left: 4px solid #007bff; padding: 10px; margin: 10px 0; background-color: #f8f9fa; }
    .compliance-ok { border-left: 4px solid #28a745; }
    .compliance-warning { border-left: 4px solid #ffc107; }
    .compliance-conflict { border-left: 4px solid #dc3545; }
    .model-response { background-color: #f0f2f6; padding: 15px; border-radius: 5px; margin: 10px 0; }
    .processing-bar { background-color: #e9ecef; border-radius: 5px; padding: 3px; margin: 10px 0; }
    .processing-progress { background-color: #007bff; height: 10px; border-radius: 3px; }
    .section-header { background-color: #f8f9fa; padding: 10px; border-radius: 5px; margin: 15px 0; }
</style>
""", unsafe_allow_html=True)

# é…ç½®Qwen APIå‚æ•°
QWEN_API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"

# ç¼“å­˜ç®¡ç†
cache = defaultdict(dict)

def get_cache_key(*args):
    """ç”Ÿæˆç¼“å­˜é”®"""
    return hashlib.md5(str(args).encode()).hexdigest()

def cached_func(func):
    """å‡½æ•°ç¼“å­˜è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        key = get_cache_key(args, kwargs)
        if key in cache[func.__name__]:
            return cache[func.__name__][key]
        result = func(*args, **kwargs)
        cache[func.__name__][key] = result
        return result
    return wrapper

@cached_func
def call_qwen_api(prompt, api_key, retry=3):
    """è°ƒç”¨Qwenå¤§æ¨¡å‹APIï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    if not api_key:
        st.error("Qwen APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·åœ¨å·¦ä¾§æ è¾“å…¥å¯†é’¥")
        return None
        
    try:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        data = {
            "model": "qwen-plus",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 4000
        }
        
        # å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨
        for attempt in range(retry):
            try:
                response = requests.post(
                    QWEN_API_URL,
                    headers=headers,
                    json=data,
                    timeout=60
                )
                
                if response.status_code == 200:
                    response_json = response.json()
                    if "choices" in response_json and len(response_json["choices"]) > 0:
                        return response_json["choices"][0]["message"]["content"]
                    else:
                        st.warning(f"APIè¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ (å°è¯• {attempt+1}/{retry})")
                else:
                    st.warning(f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code} (å°è¯• {attempt+1}/{retry})")
                
                time.sleep(2** attempt)  # æŒ‡æ•°é€€é¿
                
            except requests.exceptions.Timeout:
                st.warning(f"APIè¯·æ±‚è¶…æ—¶ (å°è¯• {attempt+1}/{retry})")
                time.sleep(2 **attempt)
            except Exception as e:
                st.warning(f"APIè°ƒç”¨å¼‚å¸¸: {str(e)} (å°è¯• {attempt+1}/{retry})")
                time.sleep(2** attempt)
                
        st.error("APIè°ƒç”¨å¤šæ¬¡å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")
        return None
        
    except Exception as e:
        st.error(f"è°ƒç”¨Qwen APIå¤±è´¥: {str(e)}")
        return None

def extract_text_from_pdf(file, progress_bar=None):
    """ä»PDFæå–æ–‡æœ¬ï¼Œæ”¯æŒå¤§æ–‡ä»¶å¤„ç†å’Œè¿›åº¦æ˜¾ç¤º"""
    try:
        pdf_reader = PdfReader(file)
        text = ""
        total_pages = len(pdf_reader.pages)
        
        for i, page in enumerate(pdf_reader.pages):
            page_text = page.extract_text() or ""
            # å¤„ç†ä¸­æ–‡ç©ºæ ¼å’Œæ¢è¡Œé—®é¢˜
            page_text = page_text.replace("  ", "").replace("\n", "").replace("\r", "")
            text += page_text
            
            # æ›´æ–°è¿›åº¦æ¡
            if progress_bar is not None:
                progress = (i + 1) / total_pages
                progress_bar.progress(progress)
                progress_bar.text(f"æå–æ–‡æœ¬: ç¬¬ {i+1}/{total_pages} é¡µ")
        
        return text
    except Exception as e:
        st.error(f"æå–æ–‡æœ¬å¤±è´¥: {str(e)}")
        return ""

def split_into_clauses(text, doc_name="æ–‡æ¡£"):
    """å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ¡æ¬¾ï¼Œå¢å¼ºä¸­æ–‡æ¡æ¬¾è¯†åˆ«å’Œå¤§æ–‡æ¡£å¤„ç†"""
    # å¢å¼ºä¸­æ–‡æ¡æ¬¾æ¨¡å¼è¯†åˆ«ï¼Œæ›´å…¨é¢çš„æ¨¡å¼åº“
    patterns = [
        # ä¸»è¦æ¡æ¬¾æ¨¡å¼
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡(?:ä¹‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)?\s*[:ï¼š]?\s*.*?)(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡(?:ä¹‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)?\s*[:ï¼š]?\s*|$)',
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ã€\s*.*?)(?=[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ã€\s*|$)',
        r'(\d+\.\s*.*?)(?=\d+\.\s*|$)',
        r'(\(\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*\)\s*.*?)(?=\(\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*\)\s*|$)',
        r'(\(\s*[1-9]+\d*\s*\)\s*.*?)(?=\(\s*[1-9]+\d*\s*\)\s*|$)',
        r'([ï¼¡-ï¼ºï½-ï½š]\.\s*.*?)(?=[ï¼¡-ï¼ºï½-ï½š]\.\s*|$)',
        r'(ã€[^ã€‘]+ã€‘\s*.*?)(?=ã€[^ã€‘]+ã€‘\s*|$)',
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¬¾\s*.*?)(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¬¾\s*|$)',
    ]
    
    # å°è¯•å„ç§æ¨¡å¼ï¼Œæ‰¾åˆ°æœ€ä½³åˆ†å‰²
    best_clauses = []
    for pattern in patterns:
        clauses = re.findall(pattern, text, re.DOTALL)
        # è¿‡æ»¤è¿‡çŸ­æ¡æ¬¾
        clauses = [clause.strip() for clause in clauses if clause.strip() and len(clause.strip()) > 10]
        if len(clauses) > len(best_clauses) and len(clauses) > 2:
            best_clauses = clauses
    
    # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿçš„æ¡æ¬¾ï¼Œè¿”å›ç»“æœ
    if best_clauses:
        return best_clauses
    
    # å°è¯•æ®µè½åˆ†å‰²ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
    paragraphs = re.split(r'[ã€‚ï¼›ï¼ï¼Ÿ]\s*', text)
    paragraphs = [p.strip() for p in paragraphs if p.strip() and len(p) > 10]
    
    # å¦‚æœæ®µè½æ•°é‡ä»ç„¶å¾ˆå°‘ï¼Œå°è¯•æŒ‰å›ºå®šé•¿åº¦åˆ†å—ï¼ˆå¤„ç†éå¸¸å¤§çš„æ–‡æ¡£ï¼‰
    if len(paragraphs) < 3 and len(text) > 5000:
        chunk_size = 1000  # æ¯ä¸ªå—å¤§çº¦1000å­—ç¬¦
        paragraphs = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
        st.warning(f"{doc_name} æ¡æ¬¾ç»“æ„ä¸æ˜æ˜¾ï¼Œå·²æŒ‰ {chunk_size} å­—ç¬¦é•¿åº¦åˆ†å—å¤„ç†")
    
    return paragraphs

@lru_cache(maxsize=1000)
def chinese_text_similarity(text1, text2):
    """è®¡ç®—ä¸­æ–‡æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œä½¿ç”¨åˆ†è¯ååŒ¹é…ï¼Œç»“æœç¼“å­˜"""
    # è¿‡æ»¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
    text1_clean = re.sub(r'[^\w\s]', '', text1)
    text2_clean = re.sub(r'[^\w\s]', '', text2)
    
    # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
    words1 = list(jieba.cut(text1_clean))
    words2 = list(jieba.cut(text2_clean))
    
    # è®¡ç®—åˆ†è¯åçš„ç›¸ä¼¼åº¦
    return SequenceMatcher(None, words1, words2).ratio()

def extract_key_terms(text):
    """æå–æ–‡æœ¬ä¸­çš„å…³é”®æœ¯è¯­ï¼Œç”¨äºå¢å¼ºåŒ¹é…"""
    terms = set()
    
    # æå–æ¡æ¬¾å·
    clause_numbers = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡', text)
    terms.update(clause_numbers)
    
    # æå–å¯èƒ½çš„å…³é”®åè¯
    nouns = re.findall(r'ã€[^ã€‘]+ã€‘', text)
    terms.update(nouns)
    
    return terms

def match_clauses(clauses1, clauses2, progress_container=None):
    """åŒ¹é…ä¸¤ä¸ªæ–‡æ¡£ä¸­çš„ç›¸ä¼¼æ¡æ¬¾ï¼Œä¼˜åŒ–ä¸­æ–‡åŒ¹é…å’Œå¤§æ–‡æ¡£å¤„ç†"""
    # é¢„å…ˆè®¡ç®—æ‰€æœ‰æ¡æ¬¾çš„å…³é”®æœ¯è¯­
    terms1 = [extract_key_terms(clause) for clause in clauses1]
    terms2 = [extract_key_terms(clause) for clause in clauses2]
    
    # å…ˆåŸºäºå…³é”®æœ¯è¯­è¿›è¡Œåˆæ­¥åŒ¹é…
    term_matches = defaultdict(list)
    for i, terms in enumerate(terms1):
        if terms:
            for j, other_terms in enumerate(terms2):
                overlap = len(terms & other_terms)
                if overlap > 0:
                    term_matches[i].append((j, overlap))
    
    matched_pairs = []
    used_indices = set()
    total = len(clauses1)
    
    for i, clause1 in enumerate(clauses1):
        # æ›´æ–°è¿›åº¦
        if progress_container is not None:
            progress = (i + 1) / total
            with progress_container:
                st.progress(progress)
                st.text(f"åŒ¹é…æ¡æ¬¾: {i+1}/{total}")
        
        best_match = None
        best_ratio = 0.25  # åŸºç¡€é˜ˆå€¼
        best_j = -1
        
        # ä¼˜å…ˆè€ƒè™‘æœ‰å…³é”®æœ¯è¯­åŒ¹é…çš„æ¡æ¬¾
        candidates = []
        if i in term_matches:
            # æŒ‰æœ¯è¯­é‡å åº¦æ’åº
            for j, _ in sorted(term_matches[i], key=lambda x: x[1], reverse=True):
                if j not in used_indices:
                    candidates.append(j)
        
        # å¦‚æœæ²¡æœ‰æœ¯è¯­åŒ¹é…ï¼Œè€ƒè™‘æ‰€æœ‰æœªåŒ¹é…çš„æ¡æ¬¾
        if not candidates:
            candidates = [j for j in range(len(clauses2)) if j not in used_indices]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        for j in candidates:
            ratio = chinese_text_similarity(clause1, clauses2[j])
            
            # å¦‚æœæœ‰å…³é”®æœ¯è¯­åŒ¹é…ï¼Œé€‚å½“æé«˜ç›¸ä¼¼åº¦åˆ†æ•°
            if i in term_matches and any(j == k for k, _ in term_matches[i]):
                ratio = min(1.0, ratio * 1.1)  # æé«˜10%
                
            if ratio > best_ratio:
                best_ratio = ratio
                best_match = clauses2[j]
                best_j = j
        
        if best_match:
            matched_pairs.append((clause1, best_match, best_ratio))
            used_indices.add(best_j)
    
    # å¤„ç†æœªåŒ¹é…çš„æ¡æ¬¾
    unmatched1 = [clause for i, clause in enumerate(clauses1) 
                 if i not in [idx for idx, _ in enumerate(matched_pairs)]]
    unmatched2 = [clause for j, clause in enumerate(clauses2) if j not in used_indices]
    
    return matched_pairs, unmatched1, unmatched2

def create_download_link(content, filename, text):
    """ç”Ÿæˆä¸‹è½½é“¾æ¥"""
    b64 = base64.b64encode(content.encode()).decode()
    return f'<a href="data:file/txt;base64,{b64}" download="{filename}">{text}</a>'

def analyze_compliance_with_qwen(clause1, clause2, filename1, filename2, api_key):
    """ä½¿ç”¨Qwenå¤§æ¨¡å‹åˆ†ææ¡æ¬¾åˆè§„æ€§ï¼Œä¼˜åŒ–ä¸­æ–‡æç¤ºè¯"""
    prompt = f"""
    è¯·ä»”ç»†åˆ†æä»¥ä¸‹ä¸¤ä¸ªä¸­æ–‡æ¡æ¬¾çš„åˆè§„æ€§ï¼Œåˆ¤æ–­å®ƒä»¬æ˜¯å¦å­˜åœ¨å†²çªï¼š
    
    {filename1} æ¡æ¬¾ï¼š{clause1}
    
    {filename2} æ¡æ¬¾ï¼š{clause2}
    
    è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”¨ä¸­æ–‡è¿›è¡Œè¯¦ç»†åˆ†æï¼š
    1. ç›¸ä¼¼åº¦è¯„ä¼°ï¼šè¯„ä¼°ä¸¤ä¸ªæ¡æ¬¾çš„ç›¸ä¼¼ç¨‹åº¦ï¼ˆé«˜/ä¸­/ä½ï¼‰
    2. å·®å¼‚ç‚¹åˆ†æï¼šç®€è¦æŒ‡å‡ºä¸¤ä¸ªæ¡æ¬¾åœ¨è¡¨è¿°ã€èŒƒå›´ã€è¦æ±‚ç­‰æ–¹é¢çš„ä¸»è¦å·®å¼‚
    3. åˆè§„æ€§åˆ¤æ–­ï¼šåˆ¤æ–­æ˜¯å¦å­˜åœ¨å†²çªï¼ˆæ— å†²çª/è½»å¾®å†²çª/ä¸¥é‡å†²çªï¼‰
    4. å†²çªåŸå› ï¼šå¦‚æœå­˜åœ¨å†²çªï¼Œè¯·å…·ä½“è¯´æ˜å†²çªçš„åŸå› å’Œå¯èƒ½å¸¦æ¥çš„å½±å“
    5. å»ºè®®ï¼šé’ˆå¯¹å‘ç°çš„é—®é¢˜ï¼Œç»™å‡ºä¸“ä¸šçš„å¤„ç†å»ºè®®
    
    åˆ†ææ—¶è¯·ç‰¹åˆ«æ³¨æ„ä¸­æ–‡æ³•å¾‹/åˆåŒæ¡æ¬¾ä¸­å¸¸ç”¨è¡¨è¿°çš„ç»†å¾®å·®åˆ«ï¼Œ
    å¦‚"åº”å½“"ä¸"å¿…é¡»"ã€"ä¸å¾—"ä¸"ç¦æ­¢"ã€"å¯ä»¥"ä¸"æœ‰æƒ"ç­‰è¯è¯­çš„åŒºåˆ«ã€‚
    """
    
    return call_qwen_api(prompt, api_key)

def analyze_standalone_clause_with_qwen(clause, doc_name, api_key):
    """ä½¿ç”¨Qwenå¤§æ¨¡å‹åˆ†æç‹¬ç«‹æ¡æ¬¾ï¼ˆæœªåŒ¹é…çš„æ¡æ¬¾ï¼‰"""
    prompt = f"""
    è¯·åˆ†æä»¥ä¸‹ä¸­æ–‡æ¡æ¬¾çš„å†…å®¹ï¼š
    
    {doc_name} ä¸­çš„æ¡æ¬¾ï¼š{clause}
    
    è¯·ç”¨ä¸­æ–‡è¯„ä¼°è¯¥æ¡æ¬¾çš„ä¸»è¦å†…å®¹ã€æ ¸å¿ƒè¦æ±‚ã€æ½œåœ¨å½±å“å’Œå¯èƒ½å­˜åœ¨çš„é—®é¢˜ï¼Œ
    å¹¶ç»™å‡ºç®€è¦åˆ†æå’Œå»ºè®®ã€‚åˆ†ææ—¶è¯·æ³¨æ„ä¸­æ–‡è¡¨è¿°çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§ã€‚
    """
    
    return call_qwen_api(prompt, api_key)

def analyze_document_structure(text, doc_name, api_key):
    """åˆ†ææ–‡æ¡£ç»“æ„ï¼Œè·å–æ–‡æ¡£æ¦‚è¿°å’Œä¸»è¦ç« èŠ‚"""
    if not api_key:
        return None
        
    prompt = f"""
    è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£çš„ç»“æ„å¹¶æä¾›æ¦‚è¿°ï¼š
    
    æ–‡æ¡£åç§°ï¼š{doc_name}
    æ–‡æ¡£å†…å®¹ï¼š{text[:3000]}  # åªå–å‰3000å­—ç¬¦è¿›è¡Œåˆ†æ
    
    è¯·æä¾›ï¼š
    1. æ–‡æ¡£ç±»å‹å’Œä¸»é¢˜æ¦‚è¿°ï¼ˆ100å­—ä»¥å†…ï¼‰
    2. ä¸»è¦ç« èŠ‚æˆ–æ¡æ¬¾åˆ†ç±»
    3. æ–‡æ¡£çš„æ ¸å¿ƒç›®çš„å’Œé€‚ç”¨èŒƒå›´
    
    åˆ†æåº”ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡ºæ–‡æ¡£çš„ç»“æ„ç‰¹ç‚¹ã€‚
    """
    
    return call_qwen_api(prompt, api_key)

def chunk_large_document(text, chunk_size=5000, overlap=500):
    """å°†å¤§æ–‡æ¡£åˆ†å—å¤„ç†"""
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        # ä¸‹ä¸€å—ä¸å½“å‰å—é‡å ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§
        start = end - overlap
        
        if start >= text_length:
            break
            
    return chunks

def analyze_single_comparison(base_clauses, compare_text, base_name, compare_name, api_key, file_index):
    """åˆ†æå•ä¸ªå¯¹æ¯”æ–‡ä»¶ä¸åŸºå‡†æ–‡ä»¶çš„åˆè§„æ€§ï¼Œæ”¯æŒå¤§æ–‡æ¡£å¤„ç†"""
    # æ£€æŸ¥æ–‡æ¡£å¤§å°ï¼Œå†³å®šæ˜¯å¦åˆ†å—å¤„ç†
    if len(compare_text) > 10000:  # è¶…è¿‡10000å­—ç¬¦çš„æ–‡æ¡£è§†ä¸ºå¤§æ–‡æ¡£
        st.info(f"{compare_name} æ˜¯ä¸€ä¸ªå¤§æ–‡æ¡£ï¼ˆ{len(compare_text)}å­—ç¬¦ï¼‰ï¼Œå°†è¿›è¡Œåˆ†å—å¤„ç†")
        chunks = chunk_large_document(compare_text)
        st.info(f"æ–‡æ¡£å·²åˆ†ä¸º {len(chunks)} ä¸ªå¤„ç†å—")
        
        all_compare_clauses = []
        for i, chunk in enumerate(chunks):
            # ä½¿ç”¨æ›´å®‰å…¨çš„keyå‘½åæ–¹å¼
            expander_key = f"chunk_exp_{file_index}_{i}_{hash(chunk)}"
            with st.expander(f"å¤„ç†å— {i+1}/{len(chunks)}", expanded=False, key=expander_key):
                chunk_clauses = split_into_clauses(chunk, f"{compare_name} (å— {i+1})")
                st.success(f"å— {i+1} è¯†åˆ«å‡º {len(chunk_clauses)} æ¡æ¡æ¬¾")
                all_compare_clauses.extend(chunk_clauses)
        
        compare_clauses = all_compare_clauses
    else:
        # åˆ†å‰²å¯¹æ¯”æ–‡ä»¶æ¡æ¬¾
        with st.spinner(f"æ­£åœ¨åˆ†æ {compare_name} çš„æ¡æ¬¾ç»“æ„..."):
            compare_clauses = split_into_clauses(compare_text, compare_name)
            st.success(f"{compare_name} æ¡æ¬¾åˆ†æå®Œæˆï¼Œè¯†åˆ«å‡º {len(compare_clauses)} æ¡æ¡æ¬¾")
    
    # åŒ¹é…æ¡æ¬¾ï¼Œæ˜¾ç¤ºè¿›åº¦
    progress_container = st.empty()
    with st.spinner(f"æ­£åœ¨åŒ¹é… {base_name} ä¸ {compare_name} çš„ç›¸ä¼¼æ¡æ¬¾..."):
        matched_pairs, unmatched_base, unmatched_compare = match_clauses(
            base_clauses, 
            compare_clauses,
            progress_container
        )
    progress_container.empty()
    
    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
    st.divider()
    col1, col2, col3, col4 = st.columns(4)
    col1.metric(f"{base_name} æ¡æ¬¾æ•°", len(base_clauses))
    col2.metric(f"{compare_name} æ¡æ¬¾æ•°", len(compare_clauses))
    col3.metric("åŒ¹é…æ¡æ¬¾æ•°", len(matched_pairs))
    col4.metric("æœªåŒ¹é…æ¡æ¬¾æ•°", len(unmatched_base) + len(unmatched_compare))
    
    # æ˜¾ç¤ºæ¡æ¬¾å¯¹æ¯”å’Œåˆè§„æ€§åˆ†æ
    st.divider()
    st.subheader(f"ğŸ“Š {compare_name} ä¸ {base_name} æ¡æ¬¾åˆè§„æ€§è¯¦ç»†åˆ†æï¼ˆQwenå¤§æ¨¡å‹ï¼‰")
    
    # åˆ›å»ºåˆ†æç»“æœçš„æ ‡ç­¾é¡µå¯¼èˆª
    tab_labels = ["å…¨éƒ¨åŒ¹é…é¡¹"]
    if len(unmatched_base) > 0:
        tab_labels.append(f"{base_name} ç‹¬æœ‰æ¡æ¬¾")
    if len(unmatched_compare) > 0:
        tab_labels.append(f"{compare_name} ç‹¬æœ‰æ¡æ¬¾")
    
    tabs = st.tabs(tab_labels)
    tab_idx = 0
    
    # åˆ†ææ¯ä¸ªåŒ¹é…å¯¹çš„åˆè§„æ€§
    with tabs[tab_idx]:
        tab_idx += 1
        
        # æ·»åŠ ç­›é€‰åŠŸèƒ½ - ä½¿ç”¨æ›´å®‰å…¨çš„keyå‘½åæ–¹å¼
        slider_key = f"sim_slider_{file_index}_{hash(str(base_clauses[:5]))}"
        min_similarity = st.slider(
            "æœ€ä½ç›¸ä¼¼åº¦ç­›é€‰", 
            0.0, 1.0, 0.0, 0.05,
            key=slider_key
        )
        filtered_pairs = [p for p in matched_pairs if p[2] >= min_similarity]
        
        st.write(f"æ˜¾ç¤º {len(filtered_pairs)} ä¸ªåŒ¹é…é¡¹ï¼ˆç­›é€‰åï¼‰")
        
        for i, (clause1, clause2, ratio) in enumerate(filtered_pairs):
            # æ ¹æ®ç›¸ä¼¼åº¦è®¾ç½®ä¸åŒé¢œè‰²æ ‡è¯†
            if ratio > 0.7:
                similarity_color = "#28a745"  # ç»¿è‰² - é«˜ç›¸ä¼¼åº¦
                similarity_label = "é«˜ç›¸ä¼¼åº¦"
            elif ratio > 0.4:
                similarity_color = "#ffc107"  # é»„è‰² - ä¸­ç›¸ä¼¼åº¦
                similarity_label = "ä¸­ç›¸ä¼¼åº¦"
            else:
                similarity_color = "#dc3545"  # çº¢è‰² - ä½ç›¸ä¼¼åº¦
                similarity_label = "ä½ç›¸ä¼¼åº¦"
            
            st.markdown(f"### åŒ¹é…æ¡æ¬¾å¯¹ {i+1}")
            st.markdown(f'<span style="color:{similarity_color};font-weight:bold">{similarity_label}: {ratio:.2%}</span>', unsafe_allow_html=True)
            
            col_a, col_b = st.columns(2)
            with col_a:
                st.markdown(f'<div class="clause-box"><strong>{base_name} æ¡æ¬¾:</strong><br>{clause1}</div>', unsafe_allow_html=True)
            with col_b:
                st.markdown(f'<div class="clause-box"><strong>{compare_name} æ¡æ¬¾:</strong><br>{clause2}</div>', unsafe_allow_html=True)
            
            # æ·»åŠ åˆ†æç»“æœæŠ˜å æ¡† - ä½¿ç”¨æ›´å®‰å…¨çš„keyå‘½åæ–¹å¼
            expander_key = f"analysis_exp_{file_index}_{i}_{hash(clause1[:20])}_{hash(clause2[:20])}"
            with st.expander("æŸ¥çœ‹Qwenå¤§æ¨¡å‹åˆè§„æ€§åˆ†æ", expanded=False, key=expander_key):
                with st.spinner("æ­£åœ¨è°ƒç”¨Qwenå¤§æ¨¡å‹è¿›è¡Œä¸­æ–‡åˆè§„æ€§åˆ†æ..."):
                    analysis = analyze_compliance_with_qwen(clause1, clause2, base_name, compare_name, api_key)
                
                if analysis:
                    st.markdown('<div class="model-response"><strong>Qwenå¤§æ¨¡å‹åˆ†æç»“æœ:</strong><br>' + analysis + '</div>', unsafe_allow_html=True)
                else:
                    st.warning("æœªèƒ½è·å–åˆè§„æ€§åˆ†æç»“æœ")
            
            st.divider()
    
    # æœªåŒ¹é…çš„æ¡æ¬¾åˆ†æ - åŸºå‡†æ–‡ä»¶ç‹¬æœ‰
    if len(unmatched_base) > 0 and tab_idx < len(tabs):
        with tabs[tab_idx]:
            tab_idx += 1
            st.markdown(f"#### {base_name} ä¸­ç‹¬æœ‰çš„æ¡æ¬¾ ({len(unmatched_base)})")
            
            # å…è®¸ç”¨æˆ·é€‰æ‹©æŸ¥çœ‹ç‰¹å®šæ¡æ¬¾ - ä½¿ç”¨æ›´å®‰å…¨çš„keyå‘½åæ–¹å¼
            select_key = f"unmatched_base_sel_{file_index}_{hash(str(unmatched_base[:5]))}"
            selected_clause = st.selectbox(
                "é€‰æ‹©è¦æŸ¥çœ‹çš„æ¡æ¬¾",
                range(len(unmatched_base)),
                format_func=lambda x: f"æ¡æ¬¾ {x+1}ï¼ˆ{min(50, len(unmatched_base[x]))}å­—ï¼‰",
                key=select_key
            )
            
            clause = unmatched_base[selected_clause]
            st.markdown(f'<div class="clause-box"><strong>æ¡æ¬¾ {selected_clause+1}:</strong><br>{clause}</div>', unsafe_allow_html=True)
            
            with st.spinner("Qwenå¤§æ¨¡å‹æ­£åœ¨åˆ†ææ­¤æ¡æ¬¾..."):
                analysis = analyze_standalone_clause_with_qwen(clause, base_name, api_key)
            
            if analysis:
                st.markdown('<div class="model-response"><strong>Qwenåˆ†æ:</strong><br>' + analysis + '</div>', unsafe_allow_html=True)
    
    # æœªåŒ¹é…çš„æ¡æ¬¾åˆ†æ - å¯¹æ¯”æ–‡ä»¶ç‹¬æœ‰
    if len(unmatched_compare) > 0 and tab_idx < len(tabs):
        with tabs[tab_idx]:
            tab_idx += 1
            st.markdown(f"#### {compare_name} ä¸­ç‹¬æœ‰çš„æ¡æ¬¾ ({len(unmatched_compare)})")
            
            # å…è®¸ç”¨æˆ·é€‰æ‹©æŸ¥çœ‹ç‰¹å®šæ¡æ¬¾ - ä½¿ç”¨æ›´å®‰å…¨çš„keyå‘½åæ–¹å¼
            select_key = f"unmatched_comp_sel_{file_index}_{hash(str(unmatched_compare[:5]))}"
            selected_clause = st.selectbox(
                "é€‰æ‹©è¦æŸ¥çœ‹çš„æ¡æ¬¾",
                range(len(unmatched_compare)),
                format_func=lambda x: f"æ¡æ¬¾ {x+1}ï¼ˆ{min(50, len(unmatched_compare[x]))}å­—ï¼‰",
                key=select_key
            )
            
            clause = unmatched_compare[selected_clause]
            st.markdown(f'<div class="clause-box"><strong>æ¡æ¬¾ {selected_clause+1}:</strong><br>{clause}</div>', unsafe_allow_html=True)
            
            with st.spinner("Qwenå¤§æ¨¡å‹æ­£åœ¨åˆ†ææ­¤æ¡æ¬¾..."):
                analysis = analyze_standalone_clause_with_qwen(clause, compare_name, api_key)
            
            if analysis:
                st.markdown('<div class="model-response"><strong>Qwenåˆ†æ:</strong><br>' + analysis + '</div>', unsafe_allow_html=True)

def main():
    """ä¸»å‡½æ•°ï¼Œæ§åˆ¶åº”ç”¨æµç¨‹"""
    st.title("ğŸ“„ Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·")
    st.write("ä¸Šä¼ åŸºå‡†PDFæ–‡æ¡£å’Œéœ€è¦å¯¹æ¯”çš„PDFæ–‡æ¡£ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨åˆ†ææ¡æ¬¾åˆè§„æ€§")
    
    # ä¾§è¾¹æ è®¾ç½®
    with st.sidebar:
        st.header("ğŸ”§ è®¾ç½®")
        api_key = st.text_input("Qwen API å¯†é’¥", type="password", help="è¯·è¾“å…¥æ‚¨çš„é˜¿é‡Œäº‘DashScope APIå¯†é’¥")
        st.markdown("""
        æç¤ºï¼šAPIå¯†é’¥å¯ä» [é˜¿é‡Œäº‘DashScopeæ§åˆ¶å°](https://dashscope.console.aliyun.com/) è·å–
        """)
    
    # ä¸»å†…å®¹åŒº
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“‹ åŸºå‡†æ–‡æ¡£ï¼ˆä¾‹å¦‚ï¼šæ ‡å‡†æ¡æ¬¾ï¼‰")
        base_file = st.file_uploader("ä¸Šä¼ åŸºå‡†PDFæ–‡ä»¶", type="pdf", key="base_file")
    
    with col2:
        st.subheader("ğŸ” å¯¹æ¯”æ–‡æ¡£ï¼ˆä¾‹å¦‚ï¼šå¾…å®¡æ ¸æ¡æ¬¾ï¼‰")
        compare_files = st.file_uploader(
            "ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªå¯¹æ¯”PDFæ–‡ä»¶", 
            type="pdf", 
            accept_multiple_files=True,
            key="compare_files"
        )
    
    # å¤„ç†åŸºå‡†æ–‡ä»¶
    if base_file is not None and compare_files:
        with st.spinner("æ­£åœ¨æå–åŸºå‡†æ–‡æ¡£æ–‡æœ¬..."):
            progress_bar = st.progress(0)
            base_text = extract_text_from_pdf(base_file, progress_bar)
            progress_bar.empty()
            
            if base_text:
                st.success(f"åŸºå‡†æ–‡æ¡£ '{base_file.name}' æ–‡æœ¬æå–å®Œæˆï¼Œå…± {len(base_text)} å­—ç¬¦")
                
                # åˆ†ææ–‡æ¡£ç»“æ„
                with st.expander("æŸ¥çœ‹æ–‡æ¡£ç»“æ„åˆ†æ", expanded=False):
                    structure_analysis = analyze_document_structure(base_text, base_file.name, api_key)
                    if structure_analysis:
                        st.markdown(structure_analysis)
                    else:
                        st.info("æœªè¿›è¡Œæ–‡æ¡£ç»“æ„åˆ†æï¼ˆAPIå¯†é’¥æœªè®¾ç½®æˆ–åˆ†æå¤±è´¥ï¼‰")
                
                # åˆ†å‰²åŸºå‡†æ¡æ¬¾
                with st.spinner("æ­£åœ¨åˆ†æåŸºå‡†æ–‡æ¡£æ¡æ¬¾ç»“æ„..."):
                    base_clauses = split_into_clauses(base_text, base_file.name)
                    st.success(f"åŸºå‡†æ–‡æ¡£æ¡æ¬¾åˆ†æå®Œæˆï¼Œè¯†åˆ«å‡º {len(base_clauses)} æ¡æ¡æ¬¾")
                
                # å¤„ç†æ¯ä¸ªå¯¹æ¯”æ–‡ä»¶
                for i, compare_file in enumerate(compare_files):
                    st.divider()
                    st.header(f"ğŸ“Š å¯¹æ¯”åˆ†æ {i+1}/{len(compare_files)}: {compare_file.name}")
                    
                    with st.spinner(f"æ­£åœ¨æå– {compare_file.name} æ–‡æœ¬..."):
                        progress_bar = st.progress(0)
                        compare_text = extract_text_from_pdf(compare_file, progress_bar)
                        progress_bar.empty()
                        
                        if compare_text:
                            st.success(f"{compare_file.name} æ–‡æœ¬æå–å®Œæˆï¼Œå…± {len(compare_text)} å­—ç¬¦")
                            
                            # åˆ†ææ–‡æ¡£ç»“æ„
                            with st.expander(f"æŸ¥çœ‹ {compare_file.name} ç»“æ„åˆ†æ", expanded=False):
                                structure_analysis = analyze_document_structure(compare_text, compare_file.name, api_key)
                                if structure_analysis:
                                    st.markdown(structure_analysis)
                                else:
                                    st.info("æœªè¿›è¡Œæ–‡æ¡£ç»“æ„åˆ†æï¼ˆAPIå¯†é’¥æœªè®¾ç½®æˆ–åˆ†æå¤±è´¥ï¼‰")
                            
                            # è¿›è¡Œæ¡æ¬¾å¯¹æ¯”åˆ†æ
                            analyze_single_comparison(
                                base_clauses,
                                compare_text,
                                base_file.name,
                                compare_file.name,
                                api_key,
                                i  # ä¼ å…¥æ–‡ä»¶ç´¢å¼•ä½œä¸ºå”¯ä¸€æ ‡è¯†
                            )
                        else:
                            st.error(f"æ— æ³•ä» {compare_file.name} ä¸­æå–æ–‡æœ¬")
            else:
                st.error(f"æ— æ³•ä»åŸºå‡†æ–‡æ¡£ '{base_file.name}' ä¸­æå–æ–‡æœ¬")

if __name__ == "__main__":
    main()
